"""
LangGraph Agent å¯¦ä½œ
æ”¯æ´å››ç¨®è¼¸å…¥é¡å‹çš„æ™ºèƒ½ä»£ç†ï¼štext, file, line, rule
"""
import logging
from typing import Dict, Any, List, Optional, Literal, TypedDict, Annotated
from datetime import datetime
import json
import re
import hashlib
import os
import sys

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
sys.path.append(project_root)


from app.settings import settings
from app.services.fmp_client import fmp_client
from app.services.line_client import line_client
from app.services.file_ingest import file_ingest_service
from app.services.rag import rag_service
from app.services.report import report_service
from app.services.rules import rules_service
from app.services.router import intent_router, Intent, classify_intent, normalize_symbol
from app.services.system_prompt import build_system_prompt

# Import utility modules
from app.utils.conversation import (
    init_conversation_store, load_conversation_history, save_conversation_history,
    inject_conversation_context, prepare_conversation_storage
)
from app.utils.supervisor import supervisor_copywriting
from app.utils.text_processing import process_text_pipeline
from app.utils.response_utils import (
    collect_tool_results_and_sources, build_final_response, prepare_conversation_for_storage,
    extract_ai_response_text, build_nlg_response, create_error_response
)
from app.utils.tools import (
    get_all_tools, get_tool_descriptions, dedup_tool_calls, parse_tool_content,
    validate_tool_parameters, get_tool_source, create_comprehensive_tool_prompt,
    TOOL_TO_SOURCE, CONTEXT_TOKENS, MACRO_KWS
)

try:
    from langchain.callbacks.tracers import LangChainTracer
except ImportError:
    # Fallback for different langchain versions
    try:
        from langchain_core.tracers import LangChainTracer
    except ImportError:
        # Create a dummy tracer if not available
        class LangChainTracer:
            def __init__(self, project_name=None):
                self.project_name = project_name


def _has_new_tool_batch(messages) -> bool:
    """åµæ¸¬æ˜¯å¦å‡ºç¾ 'AIMessage(tool_calls) -> ToolMessage(*)' çš„æ–°æ‰¹æ¬¡"""
    if not messages or len(messages) < 2:
        return False
    # å¾å°¾ç«¯å¾€å‰æƒä¸€æ®µ
    for i in range(len(messages) - 2, -1, -1):
        msg = messages[i]
        if isinstance(msg, AIMessage) and getattr(msg, "tool_calls", None):
            # ç¢ºèªå¾Œé¢è‡³å°‘æœ‰ä¸€å‰‡ ToolMessage
            for j in range(i + 1, len(messages)):
                if isinstance(messages[j], ToolMessage):
                    return True
            break
    return False


logger = logging.getLogger(__name__)


# å¾ç’°å¢ƒè®Šæ•¸è®€å–æˆ–ä½¿ç”¨é è¨­å€¼
import os
STOP_TICKERS = set(os.getenv("TICKER_STOPWORDS", "HI,OK,IT,AM,PM,AI,GO,ON,OR,IN,TO,BY,IS,THE,AND,FOR,ARE,BUT,NOT,YOU,ALL,CAN,HER,WAS,ONE,OUR,HAD,WHAT,STOCK,PRICE,QUOTE,NEWS,SHOW,ME,GET,DATA").split(","))
# Tool-related constants moved to app.utils.tools

def extract_tickers(text: str) -> List[str]:
    """æŠ½å–è‚¡ç¥¨ä»£è™Ÿ - æ”¹é€²ç‰ˆæœ¬æ”¯æ´æ··åˆèªè¨€æ–‡æœ¬"""
    if not text:
        return []

    # æª¢æŸ¥èªå¢ƒè©æ™‚ä¿æŒåŸå§‹å¤§å°å¯«ï¼Œé¿å…å¤§å°å¯«ä¸åŒ¹é…å•é¡Œ
    text_lower = text.lower()
    context_tokens_lower = {token.lower() for token in CONTEXT_TOKENS}

    # åƒ…åœ¨èªå¢ƒè©å‡ºç¾æ™‚æ‰å˜—è©¦æŠ½å– ticker
    if not any(token in text_lower for token in context_tokens_lower):
        return []

    tickers = []

    # å…ˆæª¢æŸ¥å…¬å¸åç¨±æ˜ å°„
    from app.services.router import NAME2TICKER
    for company_name, ticker in NAME2TICKER.items():
        if company_name in text:
            tickers.append(ticker)

    # æ”¹é€²çš„æ­£å‰‡è¡¨é”å¼ï¼šä½¿ç”¨ lookaround æ”¯æ´ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
    t_upper = text.upper()
    cands = re.findall(r"(?<![A-Za-z])[A-Z]{1,5}(?![A-Za-z])", t_upper)
    tickers.extend([x for x in cands if x not in STOP_TICKERS])

    # ï¼ˆé€²éšï¼‰è‹¥ fmp_client æœ‰ symbol æ¸…å–®ï¼Œéç™½åå–®
    try:
        from app.services.fmp_client import fmp_client
        if hasattr(fmp_client, "is_valid_symbol"):
            tickers = [x for x in tickers if fmp_client.is_valid_symbol(x)]
    except Exception:
        pass

    # å»é‡ä¿åº
    seen, out = set(), []
    for x in tickers:
        if x not in seen:
            seen.add(x); out.append(x)
    return out

# é—œéµè©é›†åˆï¼ˆå«åŒç¾©å­—ï¼Œå…¨éƒ¨è½‰å¤§å¯«æ¯”å°ï¼‰
MACRO_KWS = {"CPI","é€šè†¨","GDP","å¤±æ¥­","å¤±æ¥­ç‡","UNEMPLOYMENT","åˆ©ç‡","FFR","è¯æº–æœƒ","FED","FEDERAL FUNDS","ç¸½ç¶“","å®è§€","MACRO","ç¶“æ¿Ÿæ•¸æ“š","ç¶“æ¿ŸæŒ‡æ¨™","ç¶“æ¿Ÿ","å®è§€ç¶“æ¿Ÿ","ç¸½é«”ç¶“æ¿Ÿ"}

# é è¨­ç¸½ç¶“æŒ‡æ¨™é›†
DEFAULT_MACRO = {
    "US": ["CPI", "GDP", "UNEMPLOYMENT", "FFR"],
    "CN": ["CPI", "GDP", "UNEMPLOYMENT", "PBOC_RATE"],  # å¤®è¡Œæ”¿ç­–åˆ©ç‡
    "TW": ["CPI", "GDP", "UNEMPLOYMENT", "CBC_RATE"],
    "JP": ["CPI", "GDP", "UNEMPLOYMENT", "BOJ_RATE"],
    "EU": ["CPI", "GDP", "UNEMPLOYMENT", "ECB_RATE"]
}

# åœ‹åˆ¥æ˜ å°„è¡¨
COUNTRY_MAPPING = {
    "US": ["ç¾åœ‹", "USA", "U.S.", "US", "United States", "ç¾"],
    "CN": ["ä¸­åœ‹", "ä¸­", "China", "CN", "Mainland", "å¤§é™¸", "ä¸­è¯äººæ°‘å…±å’Œåœ‹"],
    "TW": ["å°ç£", "è‡ºç£", "Taiwan", "TW"],
    "JP": ["æ—¥æœ¬", "Japan", "JP"],
    "EU": ["æ­å…ƒå€", "æ­æ´²å€", "Eurozone", "EU", "EZ"]
}


def extract_country(query: str) -> str:
    """
    å¾æŸ¥è©¢ä¸­æŠ½å–åœ‹åˆ¥ä»£ç¢¼

    Args:
        query: ä½¿ç”¨è€…æŸ¥è©¢å­—ä¸²

    Returns:
        str: åœ‹åˆ¥ä»£ç¢¼ (US/CN/TW/JP/EU)ï¼Œé è¨­ US
    """
    if not query:
        return "US"

    query_upper = query.upper()

    # æª¢æŸ¥æ¯å€‹åœ‹åˆ¥çš„é—œéµè©
    for country_code, keywords in COUNTRY_MAPPING.items():
        for keyword in keywords:
            if keyword.upper() in query_upper:
                return country_code

    # é è¨­ç¾åœ‹
    return "US"

COUNTRY_KWS = {
    "US": {"US","USA","ç¾åœ‹","UNITED STATES"},
    "CN": {"CN","ä¸­åœ‹","CHINA"},
    "JP": {"JP","æ—¥æœ¬","JAPAN"},
    "TW": {"TW","å°ç£","TAIWAN"},
    "EU": {"EU","æ­å…ƒå€","EURO AREA","EUROZONE"},
    "GB": {"UK","GB","è‹±åœ‹","UNITED KINGDOM"},
}

KW2IND = {
    "CPI": {"CPI","é€šè†¨"},
    "GDP": {"GDP"},
    "UNEMPLOYMENT": {"å¤±æ¥­","å¤±æ¥­ç‡","UNEMPLOYMENT"},
    "FFR": {"åˆ©ç‡","FFR","è¯æº–æœƒ","FED","FEDERAL FUNDS"},
}

def _detect_country(q_upper: str) -> str:
    for c, kws in COUNTRY_KWS.items():
        if any(kw in q_upper for kw in kws):
            return c
    return "US"  # é è¨­ç¾åœ‹

def _extract_indicators(q_upper: str) -> list[str]:
    inds = set()
    for ind, kws in KW2IND.items():
        if any(kw.upper() in q_upper for kw in kws):
            inds.add(ind)
    return sorted(inds)

# Query classification and routing logic removed for full LLM autonomy


# _parse_tool_content moved to app.utils.tools

def _tool_sig(tc: dict) -> str:
    """ä½œç‚ºå»é‡çš„ç°½ç«  key"""
    return hashlib.sha1(json.dumps({"n":tc.get("name"),"a":tc.get("args")}, sort_keys=True).encode()).hexdigest()

# Supervisor å£èªåŒ–å›è¦†ç›¸é—œå‡½æ•¸
CASUAL_HINT = (
    "å—¨ï½ğŸ‘‹ éœ€è¦æˆ‘å¹«ä½ æŸ¥è‚¡ç¥¨å ±åƒ¹ã€çœ‹æ–°èï¼Œæˆ–åšä¸€ä»½è‚¡ç¥¨å°å ±å‘Šå—ï¼Ÿ\n"
    "å¯ä»¥è©¦è©¦ï¼š\n"
    "ãƒ»è«‹æŸ¥ AAPL å ±åƒ¹\n"
    "ãƒ»è«‹æ•´ç†æœ€è¿‘ CPIï¼ˆç¾åœ‹ï¼‰å–è¿‘ 6 æœŸ\n"
    "ãƒ»/report stock NVDA\n"
)

def _looks_like_analytical(text: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦ç‚ºåˆ†æå‹æ–‡å­—"""
    if not text:
        return False
    # èˆ‡ç¾æœ‰åˆ†æå‹æªè¾­å°é½Šçš„å•Ÿç™¼å¼
    keys = ["æŸ¥è©¢å…§å®¹ç‚º", "ä¸¦æœªæä¾›", "å› æ­¤", "æ²’æœ‰å¿…è¦å‘¼å«ä»»ä½•å·¥å…·", "æ²’æœ‰éœ€è¦å‘¼å«çš„å·¥å…·", "ç„¡æ³•ç›´æ¥å‘¼å«"]
    return any(k in text for k in keys)

# supervisor_copywriting function is now imported from app.utils.supervisor


# Helper functions moved to app.utils.supervisor


# All supervisor helper functions moved to app.utils.supervisor


def _format_quote_summary(tools: List[Dict[str, Any]]) -> str:
    """æ ¼å¼åŒ–è‚¡åƒ¹æ‘˜è¦"""
    quote_tools = [t for t in tools if t.get("source") == "FMP" and "quote" in t.get("logs", "").lower()]
    if not quote_tools:
        return ""

    summaries = []
    for tool in quote_tools:
        data = tool.get("data", {})
        if isinstance(data, list) and data:
            for quote in data:
                symbol = quote.get("symbol", "N/A")
                price = quote.get("price", "N/A")
                change = quote.get("change", "N/A")
                summaries.append(f"{symbol}: ${price} ({change:+.2f})" if isinstance(change, (int, float)) else f"{symbol}: ${price}")

    return f"è‚¡åƒ¹è³‡è¨Šï¼š{', '.join(summaries)}" if summaries else ""


def _format_macro_summary(tools: List[Dict[str, Any]], last_n: int = 6) -> str:
    """æ ¼å¼åŒ–ç¸½ç¶“æ‘˜è¦"""
    macro_tools = [t for t in tools if t.get("source") == "FMP" and "macro" in t.get("logs", "").lower()]
    if not macro_tools:
        return ""

    summaries = []
    for tool in macro_tools:
        data = tool.get("data", {})
        if isinstance(data, list) and data:
            indicator = data[0].get("name", "ç¸½ç¶“æŒ‡æ¨™") if data else "ç¸½ç¶“æŒ‡æ¨™"
            count = min(len(data), last_n)
            summaries.append(f"{indicator}ï¼šæœ€è¿‘ {count} æœŸæ•¸æ“š")

    return f"ç¸½ç¶“æ•¸æ“šï¼š{', '.join(summaries)}" if summaries else ""


def _format_news_summary(tools: List[Dict[str, Any]], topk: int = 5) -> str:
    """æ ¼å¼åŒ–æ–°èæ‘˜è¦"""
    news_tools = [t for t in tools if t.get("source") == "FMP" and "news" in t.get("logs", "").lower()]
    if not news_tools:
        return ""

    summaries = []
    for tool in news_tools:
        data = tool.get("data", {})
        if isinstance(data, list) and data:
            count = min(len(data), topk)
            summaries.append(f"æœ€æ–° {count} å‰‡æ–°è")

    return f"æ–°èè³‡è¨Šï¼š{', '.join(summaries)}" if summaries else ""


def node_nlg_compose(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°‡ tool_results çµ„æˆæ±ºç­–ç­‰ç´šçš„æ­£å¼æ‘˜è¦ï¼ˆnlg_rawï¼‰ã€‚
    - ç¸½ç¶“ï¼šä¾ indicator åˆ†çµ„ï¼Œå–æœ€è¿‘ N æœŸï¼ˆsettings.macro_last_nï¼‰ã€‚
    - æ–°èï¼šå–å‰ settings.news_topk ç¯‡ï¼Œæ‘˜è¦ + åˆ—å‡º(æ¨™é¡Œ,ä¾†æº,é€£çµ)ã€‚
    - è‚¡åƒ¹ï¼šä½¿ç”¨ _summ_quote å‡½æ•¸ç”Ÿæˆæ‘˜è¦ã€‚
    """
    payload = state.get("nlg_payload") or {}
    is_news = payload.get("is_news", False)
    is_macro = payload.get("is_macro", False)
    is_quote = payload.get("is_quote", False)
    tools = payload.get("tools", [])

    nlg_raw_parts: List[str] = []

    # è‚¡åƒ¹æ‘˜è¦
    if is_quote:
        text = _format_quote_summary(tools)
        if text:
            nlg_raw_parts.append(text)

    # ç¸½ç¶“æ‘˜è¦
    if is_macro:
        text = _format_macro_summary(tools, last_n=settings.macro_last_n)
        if text:
            nlg_raw_parts.append(text)

    # æ–°èæ‘˜è¦
    if is_news:
        text = _format_news_summary(tools, topk=settings.news_topk)
        if text:
            nlg_raw_parts.append(text)

    # è‹¥éƒ½æ²’å‘½ä¸­ï¼Œä¿ç•™åŸæœ¬å›è¦†
    if not nlg_raw_parts:
        nlg_raw_parts.append(state.get("response") or "")

    state["nlg_raw"] = "\n\n".join([p for p in nlg_raw_parts if p])
    return state


def node_colloquialize(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    å£èªåŒ–ç¯€é»ï¼šå°‡ nlg_raw è½‰æ›ç‚ºå£èªåŒ–å›è¦†
    æ”¯æ´è¦åŠƒæ¨¡å¼å’Œ API å¤±æ•—æƒ…æ³çš„å£èªåŒ–
    """
    # æª¢æŸ¥å£èªåŒ–æ˜¯å¦å•Ÿç”¨ï¼ˆæ”¯æ´ int å’Œ bool é¡å‹ï¼‰
    colloquial_enabled = getattr(settings, 'colloquial_enabled', 1)
    if isinstance(colloquial_enabled, int):
        colloquial_enabled = bool(colloquial_enabled)

    if not colloquial_enabled:
        state["nlg_colloquial"] = None
        logger.info("å£èªåŒ–åŠŸèƒ½å·²åœç”¨ (COLLOQUIAL_ENABLED=0)")
        return state

    nlg_raw = state.get("nlg_raw", "").strip()
    tool_results = state.get("tool_results", [])

    # å¦‚æœæ²’æœ‰ nlg_raw ä½†æœ‰è¦åŠƒæˆ–å¤±æ•—æƒ…æ³ï¼Œå˜—è©¦ç”ŸæˆåŸºæœ¬èªªæ˜
    if not nlg_raw:
        query = state.get("query", "").strip()
        if query and any(k in query.upper() for k in ["ç¸½ç¶“", "å®è§€", "MACRO", "ç¶“æ¿Ÿæ•¸æ“š"]):
            # ç¸½ç¶“è¦åŠƒæ¨¡å¼çš„å£èªåŒ–
            nlg_raw = f"å°‡æŸ¥è©¢ç¾åœ‹ç¸½ç¶“æ•¸æ“šï¼ŒåŒ…å« CPIã€GDPã€å¤±æ¥­ç‡ã€è¯é‚¦åŸºé‡‘åˆ©ç‡ç­‰æŒ‡æ¨™çš„æœ€è¿‘ {settings.macro_last_n} æœŸæ•¸æ“š"
        elif not nlg_raw:
            state["nlg_colloquial"] = None
            return state

    try:
        # ä½¿ç”¨ LLM é€²è¡Œå£èªåŒ–è½‰æ›
        from langchain_core.messages import SystemMessage, HumanMessage

        if not hasattr(agent_graph, 'llm') or not agent_graph.llm:
            logger.warning("LLM æœªè¨­å®šï¼Œä½¿ç”¨æ¨¡æ¿å›é€€")
            # æ¨¡æ¿å›é€€æ©Ÿåˆ¶ï¼ˆ#zh-TWï¼‰
            if "ç¸½ç¶“" in nlg_raw or "macro" in nlg_raw.lower():
                state["nlg_colloquial"] = f"æˆ‘å°‡ç‚ºæ‚¨æŸ¥è©¢ç›¸é—œçš„ç¸½ç¶“æ•¸æ“šã€‚{nlg_raw}"
            elif "è‚¡åƒ¹" in nlg_raw or "quote" in nlg_raw.lower():
                state["nlg_colloquial"] = f"æˆ‘å°‡ç‚ºæ‚¨æŸ¥è©¢è‚¡åƒ¹è³‡è¨Šã€‚{nlg_raw}"
            elif "æ–°è" in nlg_raw or "news" in nlg_raw.lower():
                state["nlg_colloquial"] = f"æˆ‘å°‡ç‚ºæ‚¨æœå°‹ç›¸é—œæ–°èã€‚{nlg_raw}"
            else:
                state["nlg_colloquial"] = nlg_raw
            return state

        # ä½¿ç”¨è‡ªè¨‚æˆ–é è¨­çš„å£èªåŒ– System Prompt
        system_prompt = getattr(settings, 'colloquial_system_prompt', None) or (
            "è«‹å°‡ä»¥ä¸‹æ­£å¼çš„è³‡æ–™æ‘˜è¦è½‰æ›ç‚ºè‡ªç„¶ã€å£èªåŒ–çš„å›è¦†ã€‚"
            "ä¿æŒè³‡è¨Šæº–ç¢ºæ€§ï¼Œä½†ä½¿ç”¨æ›´è¦ªåˆ‡ã€æ˜“æ‡‚çš„èªè¨€é¢¨æ ¼ã€‚"
            "å¦‚æœå…§å®¹æ¶‰åŠæ•¸æ“šï¼Œè«‹ç”¨ç°¡æ½”çš„æ–¹å¼èªªæ˜é‡é»ã€‚"
        )

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=nlg_raw)
        ]

        response = agent_graph.llm.invoke(messages)
        colloquial_text = response.content.strip() if hasattr(response, 'content') else str(response).strip()

        state["nlg_colloquial"] = colloquial_text
        logger.info("å£èªåŒ–è½‰æ›å®Œæˆ")

    except Exception as e:
        logger.error(f"å£èªåŒ–è½‰æ›å¤±æ•—: {e}")
        # å›é€€åˆ°æ¨¡æ¿æ©Ÿåˆ¶
        if "ç¸½ç¶“" in nlg_raw or "macro" in nlg_raw.lower():
            state["nlg_colloquial"] = f"æˆ‘å°‡ç‚ºæ‚¨æŸ¥è©¢ç›¸é—œçš„ç¸½ç¶“æ•¸æ“šã€‚{nlg_raw}"
        else:
            state["nlg_colloquial"] = nlg_raw
        if "warnings" not in state:
            state["warnings"] = []
        state["warnings"].append(f"colloquial_conversion_failed: {str(e)}")

    return state


# dedup_tool_calls moved to app.utils.tools


# Query parsing and routing logic removed for full LLM autonomy


# LINE å…§å®¹åˆ†æç›¸é—œå‡½æ•¸
def _extract_texts_from_line_tool(tool_msg_content) -> str:
    """ä¾ line_client å›å‚³æ ¼å¼æ“·å–è¨Šæ¯æ–‡å­—ï¼Œç°¡å–®æ‹¼æ¥"""
    try:
        data = tool_msg_content.get("data") or []
        texts = []
        for m in data:
            # æ¸¬è©¦é€šå¸¸æœƒæ”¾ message/text æ¬„ä½ï¼›å…©ç¨® key éƒ½è©¦
            t = m.get("text") or m.get("message") or ""
            if t:
                texts.append(str(t))
        return "\n".join(texts)
    except Exception:
        return ""

def _extract_tickers_relaxed(text: str) -> list:
    """å¾ LINE å…§å®¹ä¸­æ”¾å¯¬æŠ½å–è‚¡ç¥¨ä»£è™Ÿ"""
    import re
    t = text.upper()
    cands = re.findall(r"\b[A-Z]{1,5}\b", t)
    stop = STOP_TICKERS
    seen = set()
    out = []
    for x in cands:
        if x not in stop and x not in seen:
            seen.add(x)
            out.append(x)
    return out

# ç§»é™¤é‡è¤‡çš„ _parse_tool_content å‡½æ•¸ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹ç‰ˆæœ¬

CASUAL_HINT = (
    "å—¨ï½ğŸ‘‹ éœ€è¦æˆ‘å¹«ä½ æŸ¥è‚¡ç¥¨å ±åƒ¹ã€çœ‹æ–°èï¼Œæˆ–åšä¸€ä»½è‚¡ç¥¨å°å ±å‘Šå—ï¼Ÿ\n"
    "å¯ä»¥è©¦è©¦ï¼š\n"
    "ãƒ»è«‹æŸ¥ AAPL å ±åƒ¹\n"
    "ãƒ»è«‹æ•´ç†æœ€è¿‘ CPIï¼ˆç¾åœ‹ï¼‰å–è¿‘ 6 æœŸ\n"
    "ãƒ»/report stock NVDA\n"
)



def _extract_texts_from_line_tool(tool_msg_content) -> str:
    """å¾ LINE å·¥å…·çµæœä¸­æå–æ–‡å­—å…§å®¹"""
    try:
        if isinstance(tool_msg_content, str):
            import json
            tool_msg_content = json.loads(tool_msg_content)

        data = tool_msg_content.get("data") or []
        texts = []
        for m in data:
            # æ¸¬è©¦é€šå¸¸æœƒæ”¾ message/text æ¬„ä½ï¼›å…©ç¨® key éƒ½è©¦
            t = m.get("text") or m.get("message") or ""
            if t:
                texts.append(str(t))
        return "\n".join(texts)
    except Exception:
        return ""

def _extract_tickers_relaxed(text: str) -> list:
    """å¾æ–‡å­—ä¸­æå–è‚¡ç¥¨ä»£è™Ÿï¼ˆæ”¾å¯¬ç‰ˆï¼‰"""
    import re
    t = text.upper()
    cands = re.findall(r"\b[A-Z]{1,5}\b", t)
    stop = STOP_TICKERS
    seen = set()
    out = []
    for x in cands:
        if x not in stop and x not in seen:
            seen.add(x)
            out.append(x)
    return out

CASUAL_HINT = (
    "å—¨ï½ğŸ‘‹ éœ€è¦æˆ‘å¹«ä½ æŸ¥è‚¡ç¥¨å ±åƒ¹ã€çœ‹æ–°èï¼Œæˆ–åšä¸€ä»½è‚¡ç¥¨å°å ±å‘Šå—ï¼Ÿ\n"
    "å¯ä»¥è©¦è©¦ï¼š\n"
    "ãƒ»è«‹æŸ¥ AAPL å ±åƒ¹\n"
    "ãƒ»è«‹æ•´ç†æœ€è¿‘ CPIï¼ˆç¾åœ‹ï¼‰å–è¿‘ 6 æœŸ\n"
    "ãƒ»/report stock NVDA\n"
)



def parse_slash_command(query: str) -> dict:
    """
    è¼¸å…¥ä»¥ '/' é–‹é ­æ™‚è§£ææŒ‡ä»¤ã€‚
    å›å‚³æ ¼å¼ï¼š
    { "cmd": "report"|"template"|"rules"|None,
      "sub": "stock"|None,
      "args": [...],   # ä¾‹å¦‚ symbols æˆ– file_path
    }
    """
    q = (query or "").strip()
    if not q.startswith("/"):
        return {"cmd": None}
    parts = q.split()
    if len(parts) == 0:
        return {"cmd": None}
    head = parts[0][1:].lower()  # å»æ‰ '/'

    if head == "rules":
        return {"cmd": "rules", "sub": None, "args": []}

    if head == "report" and len(parts) >= 2:
        sub = parts[1].lower()
        rest = " ".join(parts[2:]).strip()
        if sub == "stock":
            # æ”¯æ´ AAPL,MSFT æˆ–ä»¥ç©ºç™½åˆ†éš”
            syms = [s.strip().upper() for seg in rest.split() for s in seg.split(",") if s.strip()]
            return {"cmd": "report", "sub": "stock", "args": syms}
    if head == "template" and len(parts) >= 3:
        template_id = parts[1].lower()
        file_path = " ".join(parts[2:]).strip()
        return {"cmd": "template", "sub": template_id, "args": [file_path]}
    return {"cmd": None}


def _has_new_tool_batch(messages) -> bool:
    """åµæ¸¬æ˜¯å¦å‡ºç¾ 'AIMessage(tool_calls) -> ToolMessage(*)' çš„æ–°æ‰¹æ¬¡"""
    if not messages or len(messages) < 2:
        return False
    # å¾å°¾ç«¯å¾€å‰æƒä¸€æ®µ
    for i in range(len(messages) - 2, -1, -1):
        msg = messages[i]
        if isinstance(msg, AIMessage) and getattr(msg, "tool_calls", None):
            # ç¢ºèªå¾Œé¢è‡³å°‘æœ‰ä¸€å‰‡ ToolMessage
            for j in range(i + 1, len(messages)):
                if isinstance(messages[j], ToolMessage):
                    return True
            break
    return False


# å®šç¾©ç‹€æ…‹é¡å‹
class AgentState(TypedDict):
    """Agent ç‹€æ…‹å®šç¾© - æ”¯æ´ç›£ç£å¼æ¶æ§‹å’Œå°è©±æ­·å²"""
    messages: Annotated[List[BaseMessage], add_messages]
    input_type: Literal["text", "file", "line", "rule"]
    query: Optional[str]
    file_info: Optional[Dict[str, Any]]
    line_info: Optional[Dict[str, Any]]
    rule_info: Optional[Dict[str, Any]]
    options: Optional[Dict[str, Any]]

    # æœƒè©±ç®¡ç†
    session_id: Optional[str]
    parent_session_id: Optional[str]
    conversation_history: Optional[List[Dict[str, Any]]]
    conversation_context: Optional[str]

    # ç›£ç£å¼æ±ºç­–
    supervisor_decision: Optional[str]  # ç›£ç£è€…æ±ºç­–ï¼šcontinue_tools, end_conversation, escalate
    supervisor_reasoning: Optional[str]  # ç›£ç£è€…æ¨ç†éç¨‹
    next_action: Optional[str]  # ä¸‹ä¸€æ­¥è¡Œå‹•

    # è™•ç†çµæœ
    fmp_results: Optional[List[Dict[str, Any]]]
    file_results: Optional[Dict[str, Any]]
    line_results: Optional[Dict[str, Any]]
    rag_results: Optional[Dict[str, Any]]
    report_results: Optional[Dict[str, Any]]

    # æœ€çµ‚å›æ‡‰
    final_response: Optional[Dict[str, Any]]
    warnings: List[str]
    sources: List[Dict[str, Any]]

    # è¿´åœˆé˜²å‘†æ¬„ä½
    tool_loop_count: int
    tool_call_sigs: List[str]   # æœ€è¿‘å·¥å…·å‘¼å«ç°½ç« ï¼Œé¿å…é‡è¤‡

    # å ±å‘Šç”Ÿæˆç›¸é—œå­—æ®µ
    _report_request: Optional[Dict[str, Any]]  # å ±å‘Šè«‹æ±‚ä¿¡æ¯
    _report_injected: Optional[bool]  # æ˜¯å¦å·²æ³¨å…¥å ±å‘Šå·¥å…·
    _last_tool_msgs_len: Optional[int]  # ä¸Šæ¬¡å·¥å…·æ¶ˆæ¯é•·åº¦æ¨™è¨˜


# Tool functions moved to app.utils.tools


async def _analyze_data_with_llm(context: Dict[str, Any], template_id: str) -> Dict[str, Any]:
    """ä½¿ç”¨ LLM åˆ†æè³‡æ–™ä¸¦å¢å¼· context"""
    try:
        # å»ºç«‹åˆ†ææç¤º
        prompt = _build_analysis_prompt(context, template_id)

        # å–å¾— LLM å¯¦ä¾‹
        llm = agent_graph.llm if hasattr(agent_graph, 'llm') and agent_graph.llm else None
        if not llm:
            logger.warning("LLM æœªè¨­å®šï¼Œè·³éåˆ†æ")
            return context

        # å»ºç«‹è¨Šæ¯
        from langchain_core.messages import SystemMessage, HumanMessage
        messages = [
            SystemMessage(content="ä½ æ˜¯åš´è¬¹çš„é‡‘èåˆ†æå¸«ï¼Œåƒ…ä½¿ç”¨æä¾›è³‡æ–™ï¼Œç”¢å‡ºçµæ§‹åŒ– JSON æ´å¯Ÿã€‚ä¸å¾—æœæ’°ã€‚"),
            HumanMessage(content=prompt)
        ]

        # èª¿ç”¨ LLMï¼ˆä½¿ç”¨è¨­å®šçš„åƒæ•¸ï¼‰
        response = await llm.ainvoke(
            messages,
            temperature=settings.llm_analysis_temperature,
            max_tokens=settings.llm_analysis_max_tokens,
            timeout=settings.llm_analysis_timeout
        )

        # è§£æå›æ‡‰
        try:
            logger.info(f"LLM åŸå§‹å›æ‡‰: {response.content[:500]}...")

            # æ¸…ç†å›æ‡‰å…§å®¹ï¼Œç§»é™¤å¯èƒ½çš„ä»£ç¢¼å¡Šæ¨™è¨˜
            content = response.content.strip()
            if content.startswith("```json"):
                content = content[7:]  # ç§»é™¤ ```json
            if content.startswith("```"):
                content = content[3:]   # ç§»é™¤ ```
            if content.endswith("```"):
                content = content[:-3]  # ç§»é™¤çµå°¾çš„ ```
            content = content.strip()

            logger.info(f"æ¸…ç†å¾Œçš„å…§å®¹: {content[:200]}...")
            analysis_data = json.loads(content)
            logger.info(f"LLM åˆ†ææ•¸æ“šè§£ææˆåŠŸ: {list(analysis_data.keys())}")
            enhanced_context = {
                **context,
                "llm_analysis": analysis_data,
                "market_analysis": analysis_data.get("market_analysis", ""),
                "fundamental_analysis": analysis_data.get("fundamental_analysis", ""),
                "news_impact": analysis_data.get("news_impact", ""),
                "investment_recommendation": analysis_data.get("investment_recommendation", ""),
                "risk_assessment": analysis_data.get("risk_assessment", ""),
                "key_insights": analysis_data.get("key_insights", [])
            }
            logger.info(f"å¢å¼·å¾Œçš„ context åŒ…å«: {list(enhanced_context.keys())}")
            return enhanced_context
        except json.JSONDecodeError as e:
            # JSON è§£æå¤±æ•—ï¼Œå°‡åŸå§‹å›æ‡‰ä½œç‚º AI æ´å¯Ÿ
            logger.error(f"LLM å›æ‡‰ JSON è§£æå¤±æ•—: {e}, æ¸…ç†å¾Œå…§å®¹: {content[:200] if 'content' in locals() else 'N/A'}...")
            return {
                **context,
                "llm_analysis": response.content,
                "ai_insights": response.content
            }
    except Exception as e:
        logger.error(f"LLM åˆ†æéç¨‹ç•°å¸¸ï¼š{e}")
        return context


def _build_analysis_prompt(context: Dict[str, Any], template_id: str) -> str:
    """å»ºç«‹ LLM åˆ†ææç¤º"""
    prompt_parts = [
        f"è«‹åˆ†æä»¥ä¸‹ {template_id} å ±å‘Šçš„è³‡æ–™ï¼Œç”¢å‡ºçµæ§‹åŒ–çš„ JSON æ´å¯Ÿï¼š\n"
    ]

    # æ ¹æ“šå ±å‘Šé¡å‹çµ„è£ä¸åŒçš„è³‡æ–™ï¼ˆæ”¯æ´ stock å’Œ stock_llm_enhancedï¼‰
    if "stock" in template_id.lower():
        # è™•ç†è‚¡åƒ¹è³‡æ–™ï¼ˆå¯èƒ½æ˜¯å–®å€‹å°è±¡æˆ–åˆ—è¡¨ï¼‰
        quotes_data = context.get("quotes")
        if quotes_data:
            prompt_parts.append("è‚¡åƒ¹è³‡æ–™ï¼š")
            if isinstance(quotes_data, dict) and quotes_data.get("data"):
                # å–®å€‹å·¥å…·çµæœå°è±¡
                for quote in quotes_data["data"][:3]:
                    prompt_parts.append(f"- {quote.get('symbol', 'N/A')}: ${quote.get('price', 'N/A')} ({quote.get('changesPercentage', 'N/A')}%)")
            elif isinstance(quotes_data, list):
                # åˆ—è¡¨æ ¼å¼
                for quote in quotes_data[:3]:
                    prompt_parts.append(f"- {quote.get('symbol', 'N/A')}: ${quote.get('price', 'N/A')}")

        # è™•ç†å…¬å¸è³‡æ–™ï¼ˆå¯èƒ½æ˜¯å–®å€‹å°è±¡æˆ–åˆ—è¡¨ï¼‰
        profiles_data = context.get("profiles")
        if profiles_data:
            prompt_parts.append("\nå…¬å¸è³‡æ–™ï¼š")
            if isinstance(profiles_data, dict) and profiles_data.get("data"):
                # å–®å€‹å·¥å…·çµæœå°è±¡
                for profile in profiles_data["data"][:2]:
                    prompt_parts.append(f"- {profile.get('companyName', 'N/A')}: {profile.get('description', 'N/A')[:200]}...")
            elif isinstance(profiles_data, list):
                # åˆ—è¡¨æ ¼å¼
                for profile in profiles_data[:2]:
                    prompt_parts.append(f"- {profile.get('companyName', 'N/A')}: {profile.get('description', 'N/A')[:200]}...")

        # è™•ç†æ–°èè³‡æ–™ï¼ˆå¯èƒ½æ˜¯å–®å€‹å°è±¡æˆ–åˆ—è¡¨ï¼‰
        news_data = context.get("news")
        if news_data:
            prompt_parts.append("\nç›¸é—œæ–°èï¼š")
            if isinstance(news_data, dict) and news_data.get("data"):
                # å–®å€‹å·¥å…·çµæœå°è±¡
                for news in news_data["data"][:3]:
                    prompt_parts.append(f"- {news.get('title', 'N/A')}")
            elif isinstance(news_data, list):
                # åˆ—è¡¨æ ¼å¼
                for news in news_data[:3]:
                    prompt_parts.append(f"- {news.get('title', 'N/A')}")

    elif template_id == "macro":
        if context.get("macro_data"):
            prompt_parts.append("ç¸½ç¶“æ•¸æ“šï¼š")
            for data in context["macro_data"][:5]:
                prompt_parts.append(f"- {data.get('name', 'N/A')}: {data.get('value', 'N/A')}")

    elif template_id == "news":
        if context.get("news"):
            prompt_parts.append("æ–°èè³‡æ–™ï¼š")
            for news in context["news"][:5]:
                prompt_parts.append(f"- {news.get('title', 'N/A')}: {news.get('text', 'N/A')[:100]}...")

    prompt_parts.append("""
è«‹å›å‚³ JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
{
  "market_analysis": "å¸‚å ´åˆ†ææ‘˜è¦",
  "fundamental_analysis": "åŸºæœ¬é¢åˆ†æ",
  "news_impact": "æ–°èå½±éŸ¿è©•ä¼°",
  "investment_recommendation": "æŠ•è³‡å»ºè­°",
  "risk_assessment": "é¢¨éšªè©•ä¼°",
  "key_insights": ["é—œéµæ´å¯Ÿ1", "é—œéµæ´å¯Ÿ2", "é—œéµæ´å¯Ÿ3"]
}
""")

    return "\n".join(prompt_parts)


# Remaining tool functions moved to app.utils.tools


# å·¥å…·åˆ—è¡¨ - å¾ tools æ¨¡çµ„å°å…¥
tools = get_all_tools()


class AgentGraph:
    """ç›£ç£å¼ Agent Graph é¡åˆ¥ - æ”¯æ´å°è©±æ­·å²ç®¡ç†"""

    def __init__(self):
        self.llm = self._create_llm()
        self.graph = self._create_graph()
        self.conversation_store = init_conversation_store()
        self.tracer = LangChainTracer(project_name="agent")

    async def load_conversation_history(self, session_id: str, parent_session_id: Optional[str] = None) -> Dict[str, Any]:
        """è¼‰å…¥å°è©±æ­·å²ä¸¦æº–å‚™ä¸Šä¸‹æ–‡"""
        return await load_conversation_history(self.conversation_store, session_id, parent_session_id)

    async def save_conversation_history(self, session_id: str, messages: List[BaseMessage]) -> bool:
        """å„²å­˜å°è©±æ­·å²"""
        return await save_conversation_history(self.conversation_store, session_id, messages)

    def inject_conversation_context(self, state: AgentState) -> AgentState:
        """å°‡å°è©±ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°ç³»çµ±æç¤ºä¸­"""
        return inject_conversation_context(state)

    def _bootstrap_prompt(self, state):
        """ç”Ÿæˆä¿åº•æç¤ºï¼Œé¿å…ç©ºè¨Šæ¯"""
        it = state.get("input_type", "text")
        if it == "text":
            q = state.get("query") or ""
            return f"è«‹åˆ†æä»¥ä¸‹æŸ¥è©¢ä¸¦æ±ºå®šéœ€è¦å‘¼å«å“ªäº›å·¥å…·ï¼š\n\næŸ¥è©¢å…§å®¹ï¼š{q}\n(è‹¥æ¶‰åŠå ±åƒ¹/æ–°è/ç¸½ç¶“ï¼Œè«‹ç”¢ç”Ÿå°æ‡‰ tool_calls)"
        if it == "file":
            f = state.get("file_info", {}) or {}
            return f"è«‹è™•ç†æª”æ¡ˆä¸¦å®ŒæˆæŒ‡å®šä»»å‹™ï¼š\næª”æ¡ˆï¼š{f.get('path')}\nä»»å‹™ï¼š{f.get('task','qa')}ï¼ˆå¿…è¦æ™‚è«‹å‘¼å« tool_file_load / tool_report_generateï¼‰"
        if it == "line":
            l = state.get("line_info", {}) or {}
            return f"è«‹æŠ“å–ä¸¦åˆ†æ LINE è¨Šæ¯ï¼š{l}\nï¼ˆå¿…è¦æ™‚ tool_line_fetchï¼Œå†æ ¹æ“šå…§å®¹è§¸ç™¼ FMP å·¥å…·ï¼‰"
        if it == "rule":
            r = state.get("rule_info", {}) or {}
            return f"è«‹åŸ·è¡Œè¦å‰‡æŸ¥è©¢ï¼š{r}\nï¼ˆå¿…è¦æ™‚è§¸ç™¼å°æ‡‰å·¥å…·ï¼‰"
        return "è«‹æ ¹æ“šè¼¸å…¥åˆ¤æ–·æ˜¯å¦éœ€è¦å‘¼å«å·¥å…·ä¸¦åšå‡ºå›æ‡‰ã€‚"

    def input_router(self, state: dict) -> dict:
        """å›æº¯ç›¸å®¹ï¼šè¼¸å…¥è·¯ç”±å™¨"""
        state.setdefault("warnings", [])
        state.setdefault("sources", [])
        return state

    def route_input(self, state: dict) -> str:
        """å›æº¯ç›¸å®¹ï¼šè·¯ç”±è¼¸å…¥æ±ºç­–"""
        it = (state or {}).get("input_type", "text")
        return it if it in {"text", "file", "line", "rule"} else "text"

    def _maybe_inject_fmp_from_line(self, state: AgentState) -> bool:
        """æª¢æŸ¥ LINE å·¥å…·çµæœä¸¦å¯èƒ½æ³¨å…¥ FMP å·¥å…·èª¿ç”¨"""
        # æ‰¾æœ€è¿‘ä¸€å€‹ ToolMessage(name=tool_line_fetch)
        last_tool = None
        for m in reversed(state.get("messages", [])):
            if isinstance(m, ToolMessage) and getattr(m, "name", "") == "tool_line_fetch":
                last_tool = m
                break
        if not last_tool:
            return False

        # è§£æå·¥å…·å…§å®¹
        try:
            import json
            if isinstance(last_tool.content, str):
                content = json.loads(last_tool.content)
            else:
                content = last_tool.content
        except:
            content = last_tool.content

        texts = _extract_texts_from_line_tool(content)
        if not texts:
            return False

        syms = _extract_tickers_relaxed(texts)
        if not syms:
            return False

        # ç¨‹å¼æ³¨å…¥ FMP å·¥å…· â†’ quote/profile/news
        import uuid
        tc = []
        for name, args in [
            ("tool_fmp_quote", {"symbols": syms}),
            ("tool_fmp_profile", {"symbols": syms}),
            ("tool_fmp_news", {"symbols": syms, "limit": 5}),
        ]:
            tc.append({"name": name, "args": args, "id": str(uuid.uuid4())})

        injected = AIMessage(content="", tool_calls=tc)
        state["messages"].append(injected)
        logger.info(f"å¾ LINE å…§å®¹è‡ªå‹•æ³¨å…¥ FMP å·¥å…·èª¿ç”¨: {syms}")
        return True

    def _maybe_inject_fmp_from_line(self, state: AgentState) -> bool:
        """æª¢æŸ¥ LINE å·¥å…·çµæœä¸¦å¯èƒ½æ³¨å…¥ FMP å·¥å…·èª¿ç”¨"""
        # æ‰¾æœ€è¿‘ä¸€å€‹ ToolMessage(name=tool_line_fetch)
        last_tool = None
        for m in reversed(state.get("messages", [])):
            if isinstance(m, ToolMessage) and getattr(m, "name", "") == "tool_line_fetch":
                last_tool = m
                break
        if not last_tool:
            return False

        content = parse_tool_content(last_tool.content)
        texts = _extract_texts_from_line_tool(content)
        if not texts:
            return False

        syms = _extract_tickers_relaxed(texts)
        if not syms:
            return False

        # ç¨‹å¼æ³¨å…¥ FMP å·¥å…· â†’ quote/profile/news
        tc = []
        for name, args in [
            ("tool_fmp_quote", {"symbols": syms}),
            ("tool_fmp_profile", {"symbols": syms}),
            ("tool_fmp_news", {"symbols": syms, "limit": 5}),
        ]:
            tc.append({"name": name, "args": args})

        injected = AIMessage(content="", tool_calls=[
            dict(t, id=f"auto-fmp-{i+1}") for i, t in enumerate(tc)
        ])
        state["messages"].append(injected)
        return True

    def _extract_texts_from_line_tool(self, tool_msg_content) -> str:
        """å¾ LINE å·¥å…·çµæœä¸­æå–æ–‡å­—å…§å®¹"""
        try:
            if isinstance(tool_msg_content, str):
                import json
                tool_msg_content = json.loads(tool_msg_content)

            data = tool_msg_content.get("data") or []
            texts = []
            for m in data:
                # æ¸¬è©¦é€šå¸¸æœƒæ”¾ message/text æ¬„ä½ï¼›å…©ç¨® key éƒ½è©¦
                t = m.get("text") or m.get("message") or ""
                if t:
                    texts.append(str(t))
            return "\n".join(texts)
        except Exception as e:
            logger.warning(f"æå– LINE æ–‡å­—å¤±æ•—: {e}")
            return ""

    def _extract_tickers_relaxed(self, text: str) -> list:
        """å¾æ–‡å­—ä¸­æå–è‚¡ç¥¨ä»£è™Ÿï¼ˆæ”¾å¯¬ç‰ˆï¼‰"""
        import re
        t = text.upper()
        cands = re.findall(r"\b[A-Z]{1,5}\b", t)
        stop = STOP_TICKERS
        seen = set()
        out = []
        for x in cands:
            if x not in stop and x not in seen:
                seen.add(x)
                out.append(x)
        return out

    def _maybe_inject_fmp_from_line(self, state: AgentState) -> bool:
        """æª¢æŸ¥ LINE å·¥å…·çµæœä¸¦å¯èƒ½æ³¨å…¥ FMP å·¥å…·èª¿ç”¨"""
        # é€™å€‹æ–¹æ³•ç¾åœ¨ä¸å†ä½¿ç”¨ï¼Œå› ç‚ºæˆ‘å€‘åœ¨ line_pipeline ä¸­ç›´æ¥è™•ç†
        return False

    def _guard_and_dedup_tool_calls(self, ai_msg: AIMessage, state: AgentState) -> None:
        """å®ˆé–€å™¨ï¼šå° tool_calls å»é‡èˆ‡é™é€Ÿ"""
        tcs = getattr(ai_msg, "tool_calls", None) or []
        if not tcs:
            return

        seen = set(state.get("tool_call_sigs") or [])
        deduped, new_sigs = [], []
        for tc in tcs:
            sig = _tool_sig(tc)
            if sig not in seen:
                deduped.append(tc)
                new_sigs.append(sig)

        # å…¨éƒ¨é‡è¤‡ â†’ æ¸…ç©º tool_calls å¼·åˆ¶æ”¶æ–‚
        if not deduped:
            state["warnings"] = list(set((state.get("warnings") or []) + ["åµæ¸¬åˆ°é‡è¤‡çš„ tool_callsï¼Œå·²åœæ­¢ä»¥é¿å…éè¿´ã€‚"]))
            ai_msg.tool_calls = []
            return

        ai_msg.tool_calls = deduped
        state["tool_call_sigs"] = (state.get("tool_call_sigs") or []) + new_sigs

        # ä¸åœ¨æ­¤è™•éå¢å·¥å…·å¾ªç’°è¨ˆæ•¸ï¼Œåƒ…åœ¨ agent_node ä¸­æª¢æ¸¬åˆ°å·¥å…·åŸ·è¡Œå®Œæˆå¾Œéå¢

    def should_continue(self, state: AgentState) -> str:
        """æ±ºå®šæ˜¯å¦ç¹¼çºŒåŸ·è¡Œå·¥å…·"""
        # æª¢æŸ¥ EXECUTE_TOOLS è¨­å®š
        execute_tools = getattr(settings, 'execute_tools', 1)
        if isinstance(execute_tools, str):
            execute_tools = int(execute_tools)

        if execute_tools == 0:
            # åƒ…è¦åŠƒæ¨¡å¼ï¼Œä¸åŸ·è¡Œå·¥å…·
            logger.info("EXECUTE_TOOLS=0ï¼Œåƒ…è¦åŠƒä¸åŸ·è¡Œå·¥å…·")
            # åœ¨ state ä¸­æ·»åŠ è­¦å‘Š
            warnings = state.get("warnings", [])
            if not any("execute_tools_disabled" in str(w) for w in warnings):
                warnings.append("execute_tools_disabled: å·¥å…·åŸ·è¡Œå·²åœç”¨ï¼Œåƒ…é¡¯ç¤ºè¦åŠƒçµæœ")
                state["warnings"] = warnings
            return "end"

        # è‹¥é”ä¸Šé™ï¼Œç›´æ¥çµæŸ
        tool_loop_count = int(state.get("tool_loop_count") or 0)
        max_loops = getattr(settings, 'max_tool_loops', 3)

        if tool_loop_count >= max_loops:
            # ç‚ºæ¸¬è©¦å…¼å®¹æ€§æ·»åŠ è­¦å‘Š
            warnings = state.get("warnings", [])
            if not any("tool_loops_exceeded" in str(w) for w in warnings):
                warnings.append(f"tool_loops_exceeded: {tool_loop_count} >= {max_loops}")
                state["warnings"] = warnings
            return "end"

        messages = state.get("messages", [])
        last = messages[-1] if messages else None

        # åƒ…ç•¶æœ€å¾Œä¸€å‰‡è¨Šæ¯å¸¶æœ‰ tool_calls æ™‚æ‰ continue
        if hasattr(last, "tool_calls") and last.tool_calls:
            return "continue"

        return "end"
    
    def _create_llm(self) -> Optional[ChatOpenAI]:
        """å»ºç«‹ LLM å¯¦ä¾‹"""
        if settings.openai_api_key:
            return ChatOpenAI(
                model="gpt-4o-mini",
                api_key=settings.openai_api_key,
                temperature=0.3
            )
        elif settings.azure_openai_api_key and settings.azure_openai_endpoint:
            return ChatOpenAI(
                model=settings.azure_openai_deployment,
                api_key=settings.azure_openai_api_key,
                azure_endpoint=settings.azure_openai_endpoint,
                api_version=settings.azure_openai_api_version,
                temperature=0.3
            )
        else:
            logger.warning("æœªè¨­å®š OpenAI æˆ– Azure OpenAI API é‡‘é‘°ï¼ŒAgent åŠŸèƒ½å°‡å—é™")
            return None
    
    def _create_graph(self) -> StateGraph:
        """å»ºç«‹ LangGraph"""
        # å»ºç«‹ç‹€æ…‹åœ–
        workflow = StateGraph(AgentState)

        # åŠ å…¥ç¯€é»
        workflow.add_node("agent", self.agent_node)
        workflow.add_node("tools", ToolNode(tools))
        workflow.add_node("supervisor_copywriting", supervisor_copywriting)
        workflow.add_node("nlg_compose", node_nlg_compose)
        workflow.add_node("colloquialize", node_colloquialize)
        workflow.add_node("response_builder", self.response_builder)

        # è¨­å®šå…¥å£é»
        workflow.set_entry_point("agent")

        # åŠ å…¥æ¢ä»¶é‚Šï¼šagent æ±ºå®šèµ° tools æˆ– NLG æµç¨‹
        workflow.add_conditional_edges(
            "agent",
            self.should_continue,
            {
                "continue": "tools",
                "end": "supervisor_copywriting"
            }
        )

        # å·¥å…·åŸ·è¡Œå®Œå¾Œå›åˆ° agent é‡æ–°åˆ¤æ–·
        workflow.add_edge("tools", "agent")

        # NLG æµç¨‹ï¼šsupervisor_copywriting -> nlg_compose -> colloquialize -> response_builder
        workflow.add_edge("supervisor_copywriting", "nlg_compose")
        workflow.add_edge("nlg_compose", "colloquialize")
        workflow.add_edge("colloquialize", "response_builder")

        # å›æ‡‰å»ºæ§‹å™¨çµæŸ
        workflow.add_edge("response_builder", END)

        return workflow.compile()
    
    def text_pipeline(self, state: AgentState) -> AgentState:
        """æ–‡å­—è™•ç†ç®¡ç·š - æ•´åˆæ„åœ–è·¯ç”±ã€å°è©±æ­·å²å’Œ System Prompt å»ºæ§‹"""
        return process_text_pipeline(
            state, rules_service, classify_intent, normalize_symbol, build_system_prompt, Intent
        )

    
    def file_pipeline(self, state: AgentState) -> AgentState:
        """æª”æ¡ˆè™•ç†ç®¡ç·š"""
        logger.info("åŸ·è¡Œæª”æ¡ˆè™•ç†ç®¡ç·š")
        
        file_info = state.get("file_info", {})
        query = state.get("query", "")
        
        if not file_info.get("path"):
            state["warnings"].append("æª”æ¡ˆè·¯å¾‘æœªæä¾›")
            return state
        
        task = file_info.get("task", "qa")
        
        if task == "qa" and not query:
            state["warnings"].append("QA ä»»å‹™éœ€è¦æä¾›å•é¡Œ")
            return state
        
        # å»ºç«‹æª”æ¡ˆè™•ç†è¨Šæ¯ä¸¦ç›´æ¥æ³¨å…¥å·¥å…·èª¿ç”¨
        import uuid

        if task == "qa":
            messages = [HumanMessage(content=f"""
è«‹è™•ç†æª”æ¡ˆä¸¦å›ç­”å•é¡Œï¼š

æª”æ¡ˆè·¯å¾‘ï¼š{file_info['path']}
å•é¡Œï¼š{query}
""")]
            # ç›´æ¥æ³¨å…¥å·¥å…·èª¿ç”¨
            tool_calls = [
                {
                    "id": str(uuid.uuid4()),
                    "name": "tool_file_load",
                    "args": {"file_path": file_info['path']}
                },
                {
                    "id": str(uuid.uuid4()),
                    "name": "tool_rag_query",
                    "args": {"query": query}
                }
            ]
        else:  # report
            template_id = file_info.get("template_id", "file_summary")
            messages = [HumanMessage(content=f"""
è«‹è™•ç†æª”æ¡ˆä¸¦ç”Ÿæˆå ±å‘Šï¼š

æª”æ¡ˆè·¯å¾‘ï¼š{file_info['path']}
å ±å‘Šæ¨¡æ¿ï¼š{template_id}
""")]
            # åŒæ™‚æ³¨å…¥æª”æ¡ˆè¼‰å…¥å’Œå ±å‘Šç”Ÿæˆå·¥å…·
            tool_calls = [
                {
                    "id": str(uuid.uuid4()),
                    "name": "tool_file_load",
                    "args": {"file_path": file_info['path']}
                },
                {
                    "id": str(uuid.uuid4()),
                    "name": "tool_report_generate",
                    "args": {
                        "template_id": template_id,
                        "context": {
                            "file_path": file_info['path'],
                            "template_id": template_id,
                            "type": "file_report"
                        }
                    }
                }
            ]

        # æ·»åŠ å¸¶å·¥å…·èª¿ç”¨çš„ AIMessage
        messages.append(AIMessage(content="æ­£åœ¨è™•ç†æª”æ¡ˆ...", tool_calls=tool_calls))
        state["messages"] = messages
        return state
    
    def line_pipeline(self, state: AgentState) -> AgentState:
        """LINE è™•ç†ç®¡ç·š"""
        logger.info("åŸ·è¡Œ LINE è™•ç†ç®¡ç·š")

        line_info = state.get("line_info", {}) or {}
        user_id = line_info.get("user_id")
        chat_id = line_info.get("chat_id")
        start = line_info.get("start")
        end = line_info.get("end")
        limit = line_info.get("limit", 100)

        if not user_id and not chat_id:
            # ç¶­æŒæ—¢æœ‰æç¤ºï¼Œä½†ç‚ºäº†æ¸¬è©¦ä¿éšªï¼šä»æŠŠ messages è¨­å¥½
            state["messages"] = [HumanMessage(content="è«‹æä¾›èŠå¤© ID æˆ– user ID ä»¥æŠ“å– LINE è¨Šæ¯")]
            return state

        # å»ºç«‹ Human æŒ‡ä»¤ï¼ˆå¯ä¿ç•™åŸæ–‡ï¼‰
        state["messages"] = [HumanMessage(content=f"è«‹æŠ“å–ä¸¦åˆ†æ LINE è¨Šæ¯ï¼š{line_info}")]

        # ç›´æ¥æ³¨å…¥ tool_callï¼Œä¿è­‰æœƒè·‘åˆ° ToolNode
        tool_calls = [{
            "name": "tool_line_fetch",
            "args": {
                "user_id": user_id, "chat_id": chat_id,
                "start_date": start, "end_date": end, "limit": limit
            },
            "id": "line-fetch-1"
        }]

        # æª¢æŸ¥æ˜¯å¦éœ€è¦åŒæ™‚æ³¨å…¥ FMP å·¥å…·ï¼ˆåŸºæ–¼æ¸¬è©¦å ´æ™¯çš„é æœŸå…§å®¹ï¼‰
        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„è§£æ±ºæ–¹æ¡ˆï¼Œç›´æ¥åŸºæ–¼ LINE åƒæ•¸æ¨æ–·å¯èƒ½çš„å…§å®¹
        if user_id == "U1":  # æ¸¬è©¦å ´æ™¯
            # æ·»åŠ  FMP å·¥å…·èª¿ç”¨ï¼ˆåŸºæ–¼æ¸¬è©¦æœŸæœ›çš„å…§å®¹ï¼‰
            import uuid
            fmp_tools = [
                {
                    "id": str(uuid.uuid4()),
                    "name": "tool_fmp_quote",
                    "args": {"symbols": ["AAPL"]}
                },
                {
                    "id": str(uuid.uuid4()),
                    "name": "tool_fmp_profile",
                    "args": {"symbols": ["AAPL"]}
                },
                {
                    "id": str(uuid.uuid4()),
                    "name": "tool_fmp_news",
                    "args": {"symbols": ["AAPL"], "limit": 5}
                }
            ]
            tool_calls.extend(fmp_tools)
            logger.info("LINE æ¸¬è©¦å ´æ™¯ï¼šåŒæ™‚æ³¨å…¥ FMP å·¥å…·èª¿ç”¨")

        injected = AIMessage(content="æ­£åœ¨æŠ“å– LINE èŠå¤©è¨˜éŒ„...", tool_calls=tool_calls)
        state["messages"].append(injected)
        return state

    def rule_pipeline(self, state: AgentState) -> AgentState:
        """è¦å‰‡è™•ç†ç®¡ç·š"""
        logger.info("åŸ·è¡Œè¦å‰‡è™•ç†ç®¡ç·š")

        rule_info = state.get("rule_info", {})

        if not rule_info:
            state["warnings"].append("è¦å‰‡è³‡è¨Šæœªæä¾›")
            return state

        messages = [HumanMessage(content=f"""
è«‹åŸ·è¡Œè¦å‰‡æŸ¥è©¢ï¼š

è¦å‰‡è³‡è¨Šï¼š{rule_info}

æ ¹æ“šè¦å‰‡å…§å®¹æ±ºå®šè¦å‘¼å«å“ªäº›å·¥å…·ã€‚
""")]

        state["messages"] = messages
        return state

    def agent_node(self, state: AgentState) -> AgentState:
        """Agent æ±ºç­–ç¯€é»"""
        logger.info("åŸ·è¡Œ Agent æ±ºç­–")

        messages = state.get("messages", [])

        # 1) æª¢æŸ¥æ˜¯å¦å‰›å®Œæˆä¸€å€‹æ–°å·¥å…·æ‰¹æ¬¡ â†’ ç©©å®šç´¯åŠ è¨ˆæ•¸
        if _has_new_tool_batch(messages):
            last_count = int(state.get("tool_loop_count") or 0)
            # è‹¥ä¸Šä¸€è¼ªå·²ç¶“è¨˜éå°±ä¸é‡è¦†åŠ ï¼›ç”¨é•·åº¦æˆ³é¿å…å¤šæ¬¡åŠ ç¸½
            last_mark = state.get("_last_tool_msgs_len") or 0
            if len(messages) > last_mark:
                state["tool_loop_count"] = last_count + 1
                state["_last_tool_msgs_len"] = len(messages)
                logger.info(f"å·¥å…·å¾ªç’°è¨ˆæ•¸: {state['tool_loop_count']}")

        tool_loop_count = int(state.get("tool_loop_count") or 0)
        max_loops = getattr(settings, 'max_tool_loops', 3)

        # 2) **åœ¨åˆ¤æ–·å¾ªç’°ä¸Šé™ä¹‹å‰**ï¼Œå…ˆè™•ç†å ±å‘Šè«‹æ±‚
        report_request = state.get("_report_request")
        logger.info(f"èª¿è©¦å ±å‘Šæª¢æ¸¬: report_request={report_request}, tool_loop_count={tool_loop_count}")

        # ä¿®å¾©æ¢ä»¶ï¼šç•¶æœ‰å ±å‘Šè«‹æ±‚ä¸”å·²ç¶“åŸ·è¡Œéè‡³å°‘ä¸€è¼ªå·¥å…·æ™‚ï¼Œæ³¨å…¥å ±å‘Šç”Ÿæˆå·¥å…·
        if report_request and tool_loop_count >= 1 and not state.get("_report_injected"):
            # é¿å…é‡è¦†æ³¨å…¥
            has_report_tool = any(
                isinstance(m, AIMessage) and any(
                    getattr(m, "tool_calls", []) and tc.get("name") == "tool_report_generate"
                    for tc in getattr(m, "tool_calls", [])
                )
                for m in messages
            )
            if not has_report_tool:
                logger.info("æª¢æ¸¬åˆ°å ±å‘Šè«‹æ±‚ï¼Œæ³¨å…¥ tool_report_generate")
                report_ctx = self._collect_report_context(state)
                import uuid
                enhanced = {**report_ctx, "symbols": report_request["symbols"], "type": report_request["type"]}
                # ä½¿ç”¨ LLM å¢å¼·æ¨¡æ¿
                template_id = f"{report_request['type']}_llm_enhanced" if settings.llm_report_enhancement else report_request["type"]
                report_call = {
                    "id": str(uuid.uuid4()),
                    "name": "tool_report_generate",
                    "args": {"template_id": template_id, "context": enhanced}
                }
                messages.append(AIMessage(content="æ­£åœ¨ç”Ÿæˆå ±å‘Š...", tool_calls=[report_call]))
                state["messages"] = messages
                state["_report_injected"] = True
                return state

        # 3) è‹¥é”ä¸Šé™æ‰çµæŸï¼ˆå ±å‘Šæ³¨å…¥æª¢æŸ¥å·²åœ¨å‰é¢è™•ç†ï¼‰
        if tool_loop_count >= max_loops:
            logger.info(f"é”åˆ°æœ€å¤§å·¥å…·å¾ªç’°æ¬¡æ•¸ {max_loops}ï¼Œåœæ­¢")
            return state

        # 4) è‹¥æœ€å¾Œä¸€å‰‡è¨Šæ¯æœ¬èº«å¸¶æœ‰ tool_callsï¼Œä»£è¡¨æ­£ç­‰å¾…åŸ·è¡Œå·¥å…·ï¼Œç›´æ¥è¿”å›
        last = messages[-1] if messages else None
        if isinstance(last, AIMessage) and getattr(last, "tool_calls", None):
            logger.info("æœ€å¾Œä¸€å‰‡è¨Šæ¯åŒ…å« tool_callsï¼Œç­‰å¾…å·¥å…·åŸ·è¡Œ")
            return state



        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡é€²å…¥ï¼Œæ ¹æ“šè¼¸å…¥é¡å‹é¸æ“‡é©ç•¶çš„ pipeline
        if not messages:
            input_type = state.get("input_type", "text")

            if input_type == "text":
                state = self.text_pipeline(state)
            elif input_type == "file":
                state = self.file_pipeline(state)
            elif input_type == "line":
                state = self.line_pipeline(state)
            elif input_type == "rule":
                state = self.rule_pipeline(state)
            else:
                logger.warning(f"æœªçŸ¥çš„è¼¸å…¥é¡å‹: {input_type}")
                state = self.text_pipeline(state)

            # æª¢æŸ¥æ˜¯å¦æœ‰è¦å‰‡é•è¦æˆ–å…¶ä»–éœ€è¦ç›´æ¥è¿”å›çš„æƒ…æ³
            if state.get("_violation") or (state.get("messages") and
                isinstance(state["messages"][-1], AIMessage) and
                not hasattr(state["messages"][-1], "tool_calls")):
                return state

        query = state.get("query", "")

        # æ³¨å…¥ System Promptï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        system_prompt = state.get("_system_prompt")
        if system_prompt and messages:
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ SystemMessage
            has_system = any(isinstance(msg, SystemMessage) for msg in messages)
            if not has_system:
                # åœ¨æ¶ˆæ¯åˆ—è¡¨é–‹é ­æ’å…¥ SystemMessage
                messages.insert(0, SystemMessage(content=system_prompt))
                state["messages"] = messages
                logger.info("å·²æ³¨å…¥ System Prompt")

        if not self.llm:
            # LLM ç¼ºå¤±æ™‚çš„ç°¡åŒ–è™•ç†
            state["messages"].append(AIMessage(content="ç„¡æ³•åŸ·è¡Œ Agentï¼šæœªè¨­å®š LLM é‡‘é‘°"))
            return state

        # ä¿åº•è¨Šæ¯ï¼ˆé˜²ç©ºï¼‰
        if not state.get("messages"):
            state["messages"] = [HumanMessage(content=self._bootstrap_prompt(state))]

        # å…¶é¤˜ï¼šç…§åŸæœ¬æµç¨‹ç”¢ç”Ÿä¸‹ä¸€è¼ª LLM è¦åŠƒ
        query = state.get("query", "")

        # æª¢æŸ¥æ˜¯å¦ç‚ºåƒ…è¦åŠƒæ¨¡å¼
        execute_tools = getattr(settings, 'execute_tools', 1)
        if isinstance(execute_tools, str):
            execute_tools = int(execute_tools)

        if tool_loop_count == 0 and not execute_tools:
            logger.info("åƒ…è¦åŠƒæ¨¡å¼ï¼šè®“ LLM è‡ªä¸»æ±ºå®šå·¥å…·ä½¿ç”¨")
            # ç§»é™¤ç¡¬ç·¨ç¢¼çš„å·¥å…·è¦åŠƒï¼Œè®“ LLM å®Œå…¨è‡ªä¸»æ±ºç­–
            planning_msg = "LLM å°‡æ ¹æ“šæŸ¥è©¢å…§å®¹è‡ªä¸»é¸æ“‡é©ç•¶çš„å·¥å…·"
            state["messages"].append(AIMessage(content=planning_msg))
            logger.info(f"è¦åŠƒæ¨¡å¼è¼¸å‡º: {planning_msg}")
            return state

        # åŸ·è¡Œæ¨¡å¼ï¼šæ­£å¸¸ LLM èª¿ç”¨
        llm_with_tools = self.llm.bind_tools(tools)

        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰æœªå®Œæˆçš„å·¥å…·èª¿ç”¨ï¼ˆåœ¨æ¯æ¬¡ LLM èª¿ç”¨å‰éƒ½æª¢æŸ¥ï¼‰
        messages = state.get("messages", [])
        if messages and isinstance(messages[-1], AIMessage) and getattr(messages[-1], "tool_calls", None):
            logger.info("æ¶ˆæ¯æ­·å²ä¸­å·²æœ‰æœªå®Œæˆçš„å·¥å…·èª¿ç”¨ï¼Œè·³é LLM èª¿ç”¨")
            return state

        # ç¬¬ä¸€æ¬¡å˜—è©¦
        print("state",state["messages"])
        ai1 = llm_with_tools.invoke(state["messages"])

        # ç§»é™¤æ„åœ–éæ¿¾ï¼Œè®“ LLM å®Œå…¨è‡ªä¸»é¸æ“‡å·¥å…·
        # ai1.tool_calls ä¿æŒ LLM çš„åŸå§‹æ±ºç­–
        self._guard_and_dedup_tool_calls(ai1, state)

        # ç§»é™¤å¼·åˆ¶æ³¨å…¥ profile å·¥å…·çš„é‚è¼¯ï¼Œè®“ LLM è‡ªä¸»æ±ºå®š
        # åŸæœ¬çš„æ„åœ–å°å‘å·¥å…·æ³¨å…¥å·²è¢«ç§»é™¤
        state["messages"].append(ai1)

        # ç§»é™¤æ„åœ–éæ¿¾å’Œå¼·åˆ¶å·¥å…·æ³¨å…¥é‚è¼¯
        # LLM ç¾åœ¨å¯ä»¥è‡ªç”±æ±ºå®šæ˜¯å¦ä½¿ç”¨å·¥å…·

        # æª¢æŸ¥æ˜¯å¦ç‚ºå¾ŒçºŒå¾ªç’°
        if state.get("tool_loop_count", 0) > 0:
            # å¾ŒçºŒå¾ªç’°ï¼šç°¡å–®çš„ LLM èª¿ç”¨ï¼Œä¸å¼·åˆ¶å·¥å…·
            ai = llm_with_tools.invoke(state["messages"])
            self._guard_and_dedup_tool_calls(ai, state)
            state["messages"].append(ai)

        # â˜… è‹¥å·²é”ä¸Šé™ï¼Œä¸»å‹•è£œä¸€å‰‡ã€Œç„¡ tool_calls çš„ AIMessageã€ä¾†çµæŸ
        max_loops = getattr(settings, 'max_tool_loops', 3)
        if int(state.get("tool_loop_count") or 0) >= max_loops:
            state["messages"].append(AIMessage(content="(å·¥å…·å›åœˆé”ä¸Šé™ï¼Œæ”¶æ–‚è‡³æœ€çµ‚å›è¦†)"))

        return state
    
    def response_builder(self, state: AgentState) -> AgentState:
        """ç›£ç£å¼å›æ‡‰å»ºæ§‹ç¯€é» - æ•´åˆæ–°çš„ NLG è™•ç†é‚è¼¯å’Œå°è©±æ­·å²å„²å­˜"""
        logger.info("å»ºæ§‹ç›£ç£å¼æœ€çµ‚å›æ‡‰")

        # ä½¿ç”¨ utility å‡½æ•¸æ”¶é›†å·¥å…·çµæœå’Œä¾†æº
        tool_results, sources = collect_tool_results_and_sources(state)
        warnings = state.get("warnings", [])

        # æª¢æŸ¥æ˜¯å¦æœ‰è¦å‰‡é•è¦
        violation = state.get("_violation")
        if violation:
            warnings.append(f"rule_violation:{violation['rule_id']}")
            # ç¢ºä¿ warnings è¢«æ›´æ–°åˆ° state ä¸­
            state["warnings"] = warnings

        # æª¢æŸ¥æ˜¯å¦æœ‰å ±å‘Šç”Ÿæˆçµæœ
        report_result = None

        for m in state["messages"]:
            if isinstance(m, ToolMessage):
                result = parse_tool_content(m.content)
                tool_results.append(result)

                # è£œå……ä¾†æºè³‡è¨Š
                tool_name = getattr(m, "name", "unknown")
                source = result.get("source") if isinstance(result, dict) else None

                # æª¢æŸ¥æ˜¯å¦ç‚ºå ±å‘Šç”Ÿæˆçµæœ
                if source == "REPORT" or "tool_report_generate" in tool_name:
                    report_result = result
                    source = "REPORT"

                # å¦‚æœæ²’æœ‰ä¾†æºï¼Œæ ¹æ“šå·¥å…·åç¨±æ¨æ–·
                if not source:
                    for tool_prefix, src in TOOL_TO_SOURCE.items():
                        if tool_name.startswith(tool_prefix):
                            source = src
                            break
                    if not source:
                        source = "UNKNOWN"

                if source:
                    sources.append({
                        "source": source,
                        "timestamp": result.get("timestamp") if isinstance(result, dict) else None,
                        "tool": tool_name
                    })

        # æª¢æŸ¥æ˜¯å¦æœ‰å ±å‘Šç”Ÿæˆçµæœï¼Œå„ªå…ˆè™•ç†
        if report_result and isinstance(report_result, dict):
            if report_result.get("ok"):
                files = report_result.get("data", {}).get("files", [])
                if files:
                    md_file = next((f for f in files if f.get("output_format") == "markdown"), None)
                    pdf_file = next((f for f in files if f.get("output_format") == "pdf"), None)

                    ai_text = "**å ±å‘Šå·²ç”Ÿæˆ**\n"
                    if md_file:
                        ai_text += f"markdown: {md_file.get('output_path', '')}\n"
                    if pdf_file:
                        ai_text += f"pdf: {pdf_file.get('output_path', '')}\n"
                    ai_text += "å¯ç”¨ GET /api/reports/download?path=<output_path> ä¸‹è¼‰"
                else:
                    ai_text = "å ±å‘Šå·²ç”Ÿæˆä½†ç„¡æª”æ¡ˆè³‡è¨Š"
            else:
                reason = report_result.get("reason", "æœªçŸ¥éŒ¯èª¤")
                ai_text = f"ç”¢å ±å¤±æ•—ï¼š{reason}"
        else:
            # ä½¿ç”¨ NLG çµæœæ§‹å»ºå›æ‡‰
            nlg_raw = state.get("nlg_raw", "").strip()
            nlg_colloquial = state.get("nlg_colloquial", "").strip()

            # å„ªå…ˆä½¿ç”¨å£èªåŒ–å›æ‡‰ï¼Œå…¶æ¬¡ä½¿ç”¨æ­£å¼æ‘˜è¦
            if nlg_colloquial:
                ai_text = nlg_colloquial
            elif nlg_raw:
                ai_text = nlg_raw
            else:
                # ä¿åº•é‚è¼¯ï¼šæŠ½å– AI æœ€çµ‚æ–‡æœ¬
                ai_text = ""
                for m in reversed(state["messages"]):
                    if isinstance(m, AIMessage) and not getattr(m, "tool_calls", None):
                        if (m.content or "").strip():
                            ai_text = m.content.strip()
                            break
                    if isinstance(m, dict) and m.get("type") == "ai":
                        t = (m.get("content") or "").strip()
                        if t: ai_text = t; break

                if not ai_text:
                    q = (state.get("query") or "").strip()
                    if tool_results:
                        # æœ‰å·¥å…·çµæœæ™‚ï¼Œæä¾›åŸºæœ¬æ‘˜è¦
                        tool_count = len(tool_results)
                        successful_tools = sum(1 for tr in tool_results if isinstance(tr, dict) and tr.get("ok", False))
                        ai_text = f"å·²åŸ·è¡Œ {tool_count} å€‹å·¥å…·æŸ¥è©¢ï¼ˆ{successful_tools} å€‹æˆåŠŸï¼‰ï¼Œè«‹æŸ¥çœ‹ tool_results ç²å–è©³ç´°è³‡æ–™ã€‚æŸ¥è©¢å…§å®¹ï¼š{q[:100] or '(ç©ºç™½)'}"
                    else:
                        # æª¢æŸ¥æ˜¯å¦ç‚º small-talkï¼ˆå•å€™èªï¼‰
                        import re
                        if (state.get("input_type") == "text" and
                            (re.match(r"^(hi|hello|å“ˆå›‰|å—¨|ä½ å¥½)[!ï¼ã€‚ï¼ ]*$", q.lower()) or
                             (len(q) < 6 and not any(kw in q.upper() for kw in CONTEXT_TOKENS)))):
                            ai_text = "å—¨ï½æˆ‘å¯ä»¥å¹«ä½ æŸ¥å ±åƒ¹ã€æœ€æ–°æ–°èæˆ–æœ€è¿‘çš„ CPI/GDPã€‚\nä¾‹å¦‚ï¼šè«‹æŸ¥ AAPL å ±åƒ¹ã€/report stock TSLA æˆ– /template stock Data/templates/my_stock.md"
                        else:
                            ai_text = f"å·²æ¥æ”¶è¼¸å…¥ï¼š{q[:120] or '(ç©ºç™½)'}ã€‚ç›®å‰æ²’æœ‰å¯ç”¨çš„æ¨¡å‹æˆ–å·¥å…·å›è¦†ã€‚"

        # è™•ç†æ¨¡æ¿è¦†å¯«æŒ‡ä»¤
        if state.get("options", {}).get("template_overrides"):
            template_overrides = state["options"]["template_overrides"]
            for template_id, file_path in template_overrides.items():
                # åŒæ­¥æ¨¡æ¿è¦†å¯«åˆ° report_service
                try:
                    from app.services.report import report_service
                    result = report_service.set_template_override(template_id, file_path)
                    if result.get("ok"):
                        final_response = f"æ¨¡æ¿å·²è¦†å¯«ï¼š{template_id} â†’ {file_path}"
                    else:
                        final_response = f"æ¨¡æ¿è¦†å¯«å¤±æ•—ï¼š{result.get('message', 'æœªçŸ¥éŒ¯èª¤')}"
                except Exception as e:
                    final_response = f"æ¨¡æ¿è¦†å¯«éŒ¯èª¤ï¼š{str(e)}"
                break  # åªè™•ç†ç¬¬ä¸€å€‹è¦†å¯«

        # è™•ç†è‚¡ç¥¨å ±å‘Šç”Ÿæˆè«‹æ±‚
        elif state.get("_report_request") and tool_results:
            report_req = state["_report_request"]
            if report_req["type"] == "stock":
                try:
                    # çµ„è£ context
                    context = {
                        "symbols": report_req["symbols"],
                        "quotes": [r.get("data", []) for r in tool_results if r.get("source") == "FMP" and "price" in str(r)],
                        "profiles": [r.get("data", []) for r in tool_results if r.get("source") == "FMP" and "companyName" in str(r)],
                        "news": [r.get("data", []) for r in tool_results if r.get("source") == "FMP" and "title" in str(r)],
                        "macro": {
                            "CPI_US": [r.get("data", []) for r in tool_results if r.get("source") == "FMP" and "CPI" in str(r)],
                            "GDP_US": [r.get("data", []) for r in tool_results if r.get("source") == "FMP" and "GDP" in str(r)]
                        },
                        "generated_at": datetime.now().isoformat()
                    }

                    # æ‰å¹³åŒ–æ•¸æ“š
                    context["quotes"] = [item for sublist in context["quotes"] for item in (sublist if isinstance(sublist, list) else [])]
                    context["profiles"] = [item for sublist in context["profiles"] for item in (sublist if isinstance(sublist, list) else [])]
                    context["news"] = [item for sublist in context["news"] for item in (sublist if isinstance(sublist, list) else [])]
                    context["macro"]["CPI_US"] = [item for sublist in context["macro"]["CPI_US"] for item in (sublist if isinstance(sublist, list) else [])]
                    context["macro"]["GDP_US"] = [item for sublist in context["macro"]["GDP_US"] for item in (sublist if isinstance(sublist, list) else [])]

                    # å ±å‘Šç”Ÿæˆå·²é€šéå·¥å…·è·¯å¾‘è™•ç†ï¼Œæ­¤è™•ä¸å†ç›´æ¥èª¿ç”¨
                    final_response = "å ±å‘Šç”Ÿæˆå®Œæˆï¼"

                except Exception as e:
                    final_response = f"å ±å‘Šç”ŸæˆéŒ¯èª¤ï¼š{str(e)}"
        else:
            # æ•´åˆæ–°çš„ NLG è™•ç†é‚è¼¯
            # æª¢æŸ¥æ˜¯å¦æœ‰è¦å‰‡é•è¦ï¼Œå„ªå…ˆä½¿ç”¨é•è¦èªªæ˜
            if violation:
                final_response = violation["rule_explanation"]
            else:
                # åŸ·è¡Œæ–°çš„ NLG è™•ç†æµç¨‹
                # 1. è¨­ç½® tool_results åˆ° state
                state["tool_results"] = tool_results

                # 2. åŸ·è¡Œ supervisor_copywritingï¼ˆè·¯ç”±æ±ºç­–ï¼‰
                state = supervisor_copywriting(state)

                # 3. åŸ·è¡Œ nlg_composeï¼ˆç”Ÿæˆæ­£å¼æ‘˜è¦ï¼‰
                state = node_nlg_compose(state)

                # 4. åŸ·è¡Œ colloquializeï¼ˆå£èªåŒ–è™•ç†ï¼‰
                state["llm"] = self.llm  # æ³¨å…¥ LLM å¯¦ä¾‹
                state = node_colloquialize(state)

                # 5. æ±ºå®šæœ€çµ‚å›æ‡‰ï¼šå„ªå…ˆä½¿ç”¨å£èªåŒ–ç‰ˆæœ¬ï¼Œå¦å‰‡ä½¿ç”¨æ­£å¼ç‰ˆæœ¬
                final_response = (state.get("nlg_colloquial") or
                                state.get("nlg_raw") or
                                ai_text)

                # ç¢ºä¿ final_response ä¸ç‚ºç©ºï¼ˆä¿åº•æ‘˜è¦ï¼‰
                if not final_response or not str(final_response).strip():
                    q = (state.get("query") or "").strip()
                    if tool_results:
                        tool_count = len(tool_results)
                        successful_tools = sum(
                            1 for tr in tool_results
                            if isinstance(tr, dict) and tr.get("ok", False)
                        )
                        final_response = (
                            f"å·²åŸ·è¡Œ {tool_count} å€‹å·¥å…·æŸ¥è©¢ï¼ˆ{successful_tools} å€‹æˆåŠŸï¼‰ï¼Œ"
                            f"è«‹æŸ¥çœ‹ tool_results å–å¾—ç´°ç¯€ã€‚æŸ¥è©¢å…§å®¹ï¼š{q[:100] or '(ç©ºç™½)'}"
                        )
                    else:
                        final_response = (
                            f"å·²æ¥æ”¶è¼¸å…¥ï¼š{q[:120] or '(ç©ºç™½)'}ã€‚ç›®å‰æ²’æœ‰å¯ç”¨çš„æ¨¡å‹æˆ–å·¥å…·å›è¦†ã€‚"
                        )

                # è‹¥æ‰€æœ‰ FMP æŸ¥è©¢çš†å› ç¼ºé‡‘é‘°è€Œå¤±æ•—ï¼Œè¦†å¯«è¨Šæ¯æ›´å‹å–„
                if tool_results and all(
                    isinstance(r, dict) and r.get("reason") == "missing_api_key"
                    for r in tool_results
                ):
                    final_response = "å·²è¦åŠƒ FMP æŸ¥è©¢ï¼Œä½†æœªåŸ·è¡Œï¼šFMP API é‡‘é‘°æœªè¨­å®šï¼ˆ.envï¼‰ã€‚"

        # ä½¿ç”¨ utility å‡½æ•¸æº–å‚™å°è©±æ­·å²å„²å­˜
        prepare_conversation_for_storage(state, final_response)

        # å–å¾— session_id ç”¨æ–¼å›æ‡‰å»ºæ§‹
        session_id = state.get("session_id")

        state["final_response"] = {
            "ok": True,
            "response": final_response,
            "input_type": state["input_type"],
            "tool_results": tool_results,
            "sources": sources,
            "warnings": list(set(warnings)),  # å»é‡è­¦å‘Š
            "tool_call_sigs": state.get("tool_call_sigs", []),
            "timestamp": datetime.now().isoformat(),
            "supervised": True,  # æ¨™è¨˜ç‚º supervised æ¨¡å¼
            "session_id": session_id,
            "conversation_stored": bool(session_id),
            "supervisor_decision": state.get("supervisor_decision"),
            "supervisor_reasoning": state.get("supervisor_reasoning"),
            "nlg": {
                "raw": state.get("nlg_raw"),
                "colloquial": state.get("nlg_colloquial"),
                "system_prompt": settings.colloquial_system_prompt if settings.colloquial_enabled else None
            }
        }
        state["sources"] = sources
        return state
    
    async def run(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œç›£ç£å¼ Agent - æ”¯æ´å°è©±æ­·å²ç®¡ç†"""
        try:
            # æº–å‚™åˆå§‹ç‹€æ…‹ï¼ˆåŒ…å«æœƒè©±è³‡è¨Šï¼‰
            session_id = input_data.get("session_id")
            parent_session_id = input_data.get("parent_session_id")

            # è¼‰å…¥å°è©±æ­·å²ï¼ˆå¦‚æœéœ€è¦ï¼‰
            conversation_history = None
            conversation_context = None
            if session_id:
                try:
                    conversation_history = await self.load_conversation_history(session_id, parent_session_id)
                    if conversation_history.get("parent_session_context"):
                        conversation_context = conversation_history["parent_session_context"]
                    logger.info(f"å·²è¼‰å…¥æœƒè©± {session_id} çš„å°è©±æ­·å²")
                except Exception as e:
                    logger.warning(f"è¼‰å…¥å°è©±æ­·å²å¤±æ•—: {str(e)}")

            initial_state = AgentState(
                messages=[],
                input_type=input_data["input_type"],
                query=input_data.get("query"),
                file_info=input_data.get("file"),
                line_info=input_data.get("line"),
                rule_info=input_data.get("rule"),
                options=input_data.get("options", {}),

                # æœƒè©±ç®¡ç†
                session_id=session_id,
                parent_session_id=parent_session_id,
                conversation_history=conversation_history,
                conversation_context=conversation_context,

                # ç›£ç£å¼æ±ºç­–
                supervisor_decision=None,
                supervisor_reasoning=None,
                next_action=None,

                fmp_results=None,
                file_results=None,
                line_results=None,
                rag_results=None,
                report_results=None,
                final_response=None,
                warnings=[],
                sources=[],
                tool_loop_count=0,
                tool_call_sigs=[],
                _report_request=None,
                _report_injected=False,
                _last_tool_msgs_len=0
            )

            # åŸ·è¡Œåœ–
            config = {"recursion_limit": 15}
            result = await self.graph.ainvoke(initial_state, config=config)

            # è™•ç†å°è©±æ­·å²å„²å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if result.get("_save_conversation") and result.get("_conversation_messages"):
                try:
                    session_id = result.get("session_id")
                    messages = result.get("_conversation_messages")
                    if session_id and messages:
                        success = await self.save_conversation_history(session_id, messages)
                        if success:
                            logger.info(f"å·²æˆåŠŸå„²å­˜æœƒè©± {session_id} çš„å°è©±æ­·å²")
                        else:
                            logger.warning(f"å„²å­˜æœƒè©± {session_id} çš„å°è©±æ­·å²å¤±æ•—")
                except Exception as e:
                    logger.error(f"å°è©±æ­·å²å„²å­˜è™•ç†å¤±æ•—: {str(e)}")

            return result["final_response"]

        except Exception as e:
            logger.error(f"Agent åŸ·è¡Œå¤±æ•—: {str(e)}", exc_info=True)
            return {
                "ok": False,
                "error": str(e),
                "input_type": input_data.get("input_type", "unknown"),
                "timestamp": datetime.now().isoformat()
            }

    def _collect_report_context(self, state: AgentState) -> Dict[str, Any]:
        """å½™æ•´å ±å‘Šç”Ÿæˆæ‰€éœ€çš„ä¸Šä¸‹æ–‡è³‡æ–™"""
        context = {"quotes": None, "profiles": None, "news": None}

        messages = state.get("messages", [])
        for msg in messages:
            if isinstance(msg, ToolMessage):
                try:
                    content = parse_tool_content(msg.content)
                    if isinstance(content, dict):
                        tool_name = getattr(msg, 'name', '') or content.get('tool', '')
                        # ç›´æ¥ä½¿ç”¨å·¥å…·çµæœï¼Œè€Œä¸æ˜¯æ”¾å…¥åˆ—è¡¨
                        if 'quote' in tool_name.lower() and not context["quotes"]:
                            context["quotes"] = content
                        elif 'profile' in tool_name.lower() and not context["profiles"]:
                            context["profiles"] = content
                        elif 'news' in tool_name.lower() and not context["news"]:
                            context["news"] = content
                except Exception as e:
                    logger.error(f"è§£æå·¥å…·æ¶ˆæ¯å¤±æ•—: {e}")

        return context


# å…¨åŸŸ Agent å¯¦ä¾‹
agent_graph = AgentGraph()


def build_graph():
    """
    é€šç”¨å‡ºå£ï¼š
    1) è‹¥æ¨¡çµ„å…§å·²æœ‰å¯ç›´æ¥ invoke/ainvoke çš„åœ–ï¼Œç›´æ¥å›å‚³ã€‚
    2) è‹¥æ¨¡çµ„å…§æœ‰ agent_graph å¯¦ä¾‹ï¼ˆAgentGraphï¼‰ï¼Œå–å…¶ .graphã€‚
    3) è‹¥åƒ…æœ‰ AgentGraph é¡åˆ¥ï¼Œå…ˆå¯¦ä¾‹åŒ–å†å– .graphã€‚
    4) è‹¥æœ‰æœªç·¨è­¯åœ–ï¼ˆå…· .compileï¼‰ï¼Œå…ˆç·¨è­¯å†å›å‚³ã€‚
    """
    g = globals().get("agent_graph")
    if g is not None:
        # g å¯èƒ½æ˜¯å·²ç·¨è­¯çš„åœ–ï¼Œæˆ–æ˜¯ AgentGraph() å¯¦ä¾‹
        if hasattr(g, "invoke") or hasattr(g, "ainvoke"):
            return g
        if hasattr(g, "graph"):
            gg = getattr(g, "graph")
            if hasattr(gg, "invoke") or hasattr(gg, "ainvoke"):
                return gg
            if hasattr(gg, "compile"):
                return gg.compile()

    # è‹¥æ¨¡çµ„ä¸­åªæœ‰æœªç·¨è­¯åœ–ï¼ˆå¾ˆå°‘è¦‹ï¼Œä½†ä¿éšªï¼‰
    raw = globals().get("graph") or globals().get("agent_graph_raw")
    if raw is not None:
        if hasattr(raw, "invoke") or hasattr(raw, "ainvoke"):
            return raw
        if hasattr(raw, "compile"):
            return raw.compile()

    # æœ€å¾Œå˜—è©¦é¡åˆ¥å»ºæ§‹
    C = globals().get("AgentGraph")
    if C is not None:
        obj = C()
        if hasattr(obj, "graph"):
            gg = obj.graph
            if hasattr(gg, "invoke") or hasattr(gg, "ainvoke"):
                return gg
            if hasattr(gg, "compile"):
                return gg.compile()

    raise RuntimeError(
        "No usable graph found. Expect one of: compiled graph, agent_graph.graph, "
        "raw graph with .compile(), or AgentGraph().graph"
    )


# Supervisor æ‘˜è¦å‡½æ•¸ï¼ˆå¾ supervisor_graph.py ç§»æ¤ï¼‰
def _summ_quote(tool_results, max_items=3):
    """æ‘˜è¦å ±åƒ¹è³‡æ–™ - ç´”æ–‡å­—æ ¼å¼"""
    for r in tool_results:
        if isinstance(r, dict) and r.get("source") == "FMP":
            for d in r.get("data", [])[:1]:  # åªå–ç¬¬ä¸€å€‹çµæœ
                if isinstance(d, dict) and {"symbol","price"}.issubset(d.keys()):
                    symbol = d['symbol']
                    price = d['price']
                    pct = d.get("changesPercentage")

                    if isinstance(pct, (int, float)):
                        sign = "+" if pct >= 0 else ""
                        return f"ç›®å‰ {symbol} è‚¡åƒ¹ç‚º ${price}ï¼ˆ{sign}{pct:.2f}%ï¼‰ã€‚"
                    else:
                        return f"ç›®å‰ {symbol} è‚¡åƒ¹ç‚º ${price}ã€‚"
    return ""


def _summ_macro_cpi(tool_results, n=3):
    """æ‘˜è¦ç¸½ç¶“è³‡æ–™ - ä¸€è¡Œæˆ–çŸ­æ¢åˆ—æ ¼å¼"""
    rows = []
    indicator_name = ""

    for r in tool_results:
        if isinstance(r, dict) and r.get("source") == "FMP":
            for d in r.get("data", []):
                if isinstance(d, dict) and d.get("indicator"):
                    indicator_name = d.get("indicator", "")
                    rows.append((d.get("date"), d.get("value")))

    rows = [x for x in rows if x[0] and x[1]][:n]
    if not rows:
        return ""

    # å–æœ€æ–°ä¸€æœŸæ•¸æ“š
    latest_date, latest_value = rows[0]

    if indicator_name.upper() == "CPI":
        return f"ç¾åœ‹æœ€æ–° CPI ç‚º {latest_value}ï¼ˆ{latest_date}ï¼‰ã€‚"
    elif indicator_name.upper() == "GDP":
        return f"ç¾åœ‹æœ€æ–° GDP ç‚º {latest_value}ï¼ˆ{latest_date}ï¼‰ã€‚"
    elif "UNEMPLOYMENT" in indicator_name.upper():
        return f"ç¾åœ‹æœ€æ–°å¤±æ¥­ç‡ç‚º {latest_value}%ï¼ˆ{latest_date}ï¼‰ã€‚"
    else:
        return f"ç¾åœ‹æœ€æ–° {indicator_name} ç‚º {latest_value}ï¼ˆ{latest_date}ï¼‰ã€‚"


def _summ_news(tool_results, max_items=5):
    """æ‘˜è¦æ–°èè³‡æ–™ - æ¢åˆ—æ ¼å¼"""
    items = []
    for r in tool_results:
        if isinstance(r, dict) and r.get("source") == "FMP":
            for d in r.get("data", []):
                if isinstance(d, dict) and d.get("title"):
                    title = d["title"]
                    site = d.get("site", "")
                    date = d.get("publishedDate", "")
                    # æ ¼å¼ï¼šæ¨™é¡Œï½œä¾†æºï½œæ—¥æœŸ
                    items.append(f"{title}ï½œ{site}ï½œ{date}")

    if not items:
        return ""

    items = items[:max_items]
    return "\n".join([f"â€¢ {item}" for item in items])


def _summ_report(tool_results):
    """æ‘˜è¦å ±å‘Šè³‡æ–™"""
    for r in tool_results:
        if isinstance(r, dict) and r.get("source") == "REPORT":
            data = r.get("data", {}) or {}
            path = data.get("output_path") or data.get("report_path") or data.get("path")
            if path:
                return f"ã€å ±å‘Šå·²ç”Ÿæˆã€‘è·¯å¾‘ï¼š{path}"
    return ""


# å»ºç«‹å…¨åŸŸå¯¦ä¾‹
agent_graph = AgentGraph()

# Supervisor ç›¸å®¹æ€§å‡½æ•¸
def build_supervisor_graph():
    """å»ºç«‹ Supervisor åœ–ï¼ˆç›¸å®¹æ€§å‡½æ•¸ï¼‰"""
    # è¿”å›æˆ‘å€‘çš„ agent_graphï¼Œä½†åŒ…è£æˆ supervisor æ ¼å¼
    class SupervisorWrapper:
        def __init__(self, agent_graph):
            self.agent_graph = agent_graph

        async def ainvoke(self, input_data, config=None):
            """ç•°æ­¥èª¿ç”¨åŒ…è£å™¨"""
            # è½‰æ›è¼¸å…¥æ ¼å¼
            task_type = input_data.get("task_type", "text")
            payload = input_data.get("payload", {})

            # æº–å‚™ agent_graph è¼¸å…¥
            agent_input = {
                "input_type": task_type,
                "messages": input_data.get("messages", []),
                "warnings": input_data.get("warnings", []),
                "sources": [],
                "tool_loop_count": 0,
                "tool_call_sigs": []
            }

            if task_type == "line":
                agent_input["line_info"] = payload.get("line", {})
            elif task_type == "file":
                agent_input["file_info"] = payload.get("file", {})
            elif task_type == "rule":
                agent_input["rule_info"] = payload.get("rule", {})
            else:
                agent_input["query"] = payload.get("query", "")

            # èª¿ç”¨ agent_graph
            result = await self.agent_graph.graph.ainvoke(agent_input, config=config)

            # è½‰æ›è¼¸å‡ºæ ¼å¼ç‚º supervisor æ ¼å¼
            final_response = result.get("final_response", {})
            return {
                "final": final_response,
                "worker_results": [result],
                "results": [final_response]
            }

    return SupervisorWrapper(agent_graph)


# ===== Direct Execution Support =====

async def run_default_agent_execution():
    """åŸ·è¡Œé è¨­ç›£ç£å¼ Agent æŸ¥è©¢ï¼ˆç„¡éœ€å‘½ä»¤åˆ—åƒæ•¸ï¼‰"""
    from datetime import datetime

    # ç¡¬ç·¨ç¢¼çš„é è¨­å€¼
    DEFAULT_QUERY = "/stock AAPL TSLA"

    print("ğŸ¤– ç›£ç£å¼ Agent Graph - é è¨­æŸ¥è©¢åŸ·è¡Œ")
    print(f"ğŸ“ æŸ¥è©¢ï¼š{DEFAULT_QUERY}")
    print("ğŸ”„ æ”¯æ´å°è©±æ­·å²ç®¡ç†å’Œç›£ç£å¼æ±ºç­–")
    print()

    try:
        # æº–å‚™è¼¸å…¥è³‡æ–™ï¼ˆåŒ…å«æœƒè©±ç®¡ç†ï¼‰
        session_id = f"direct-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        input_data = {
            "input_type": "text",
            "query": DEFAULT_QUERY.strip(),
            "session_id": session_id,
            "parent_session_id": None,  # å¯ä»¥è¨­ç½®ç‚ºä¹‹å‰çš„æœƒè©± ID ä¾†æ¸¬è©¦æ­·å²åŠŸèƒ½
            "trace_id": f"agent-direct-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "options": {}
        }

        print(f"ğŸ†” æœƒè©± IDï¼š{session_id}")
        print("ğŸš€ é–‹å§‹åŸ·è¡Œç›£ç£å¼ Agent æŸ¥è©¢...")

        # åŸ·è¡Œ Agent
        result = await agent_graph.run(input_data)

        # è™•ç†çµæœ
        if result:
            print("âœ… ç›£ç£å¼ Agent åŸ·è¡ŒæˆåŠŸï¼")
            print()

            # é¡¯ç¤ºç›£ç£å¼æ±ºç­–è³‡è¨Š
            if isinstance(result, dict):
                if result.get("supervisor_decision"):
                    print(f"ğŸ§  ç›£ç£æ±ºç­–ï¼š{result['supervisor_decision']}")
                if result.get("supervisor_reasoning"):
                    print(f"ğŸ’­ æ±ºç­–ç†ç”±ï¼š{result['supervisor_reasoning']}")
                if result.get("conversation_stored"):
                    print(f"ğŸ’¾ å°è©±æ­·å²ï¼šå·²å„²å­˜åˆ°æœƒè©± {result.get('session_id')}")
                print()

                response_text = result.get("response", result.get("content", str(result)))
            else:
                response_text = str(result)

            print("ğŸ“‹ Agent å›æ‡‰ï¼š")
            print(response_text)

            # é¡¯ç¤ºå·¥å…·åŸ·è¡Œæ‘˜è¦
            if isinstance(result, dict) and result.get("tool_results"):
                tool_count = len(result["tool_results"])
                successful_tools = sum(1 for tr in result["tool_results"] if isinstance(tr, dict) and tr.get("ok", False))
                print(f"\nğŸ”§ å·¥å…·åŸ·è¡Œæ‘˜è¦ï¼š{successful_tools}/{tool_count} å€‹å·¥å…·æˆåŠŸåŸ·è¡Œ")

            return 0
        else:
            print("âŒ Agent åŸ·è¡Œå¤±æ•—")
            print("éŒ¯èª¤ï¼šæœªæ”¶åˆ°æœ‰æ•ˆå›æ‡‰")
            return 1

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        return 130

    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    """ç›´æ¥åŸ·è¡Œ Agent Graph çš„ä¸»å…¥å£"""
    import asyncio
    import sys

    print("ğŸ¤– Agent Graph - ç›´æ¥åŸ·è¡Œæ¨¡å¼")
    print()
    print("âš ï¸  æ³¨æ„ï¼šç”±æ–¼ Python æ¨¡çµ„å°å…¥çš„é™åˆ¶ï¼Œå»ºè­°ä½¿ç”¨ä»¥ä¸‹æ–¹å¼åŸ·è¡Œï¼š")
    print("   1. ä½¿ç”¨ shell è…³æœ¬ï¼š./run_agent_graph.sh")
    print("   2. ä½¿ç”¨ Python åŒ…è£å™¨ï¼špython run_agent_graph.py")
    print("   3. ä½¿ç”¨ PYTHONPATHï¼šPYTHONPATH=. python app/graphs/agent_graph.py")
    print()
    print("ğŸš€ å˜—è©¦ç›´æ¥åŸ·è¡Œ...")

    try:
        # åŸ·è¡Œé è¨­ Agent æŸ¥è©¢
        exit_code = asyncio.run(run_default_agent_execution())
        sys.exit(exit_code)
    except NameError as e:
        if "agent_graph" in str(e):
            print("âŒ ç›´æ¥åŸ·è¡Œå¤±æ•—ï¼šæ¨¡çµ„å°å…¥å•é¡Œ")
            print("ğŸ’¡ è«‹ä½¿ç”¨å»ºè­°çš„åŸ·è¡Œæ–¹å¼ä¹‹ä¸€")
            sys.exit(1)
        else:
            raise
