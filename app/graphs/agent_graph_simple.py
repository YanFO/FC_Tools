"""
ç°¡åŒ–ç‰ˆ LangGraph Agent å¯¦ä½œ
ç²¾ç°¡ç‰ˆçš„ä»£ç†åœ–ï¼Œå…è¨± LLM è‡ªä¸»æ±ºç­–åŒæ™‚ä¿æŒæ ¸å¿ƒåŠŸèƒ½ã€‚

ä¸»è¦ç°¡åŒ–é …ç›®ï¼š
1. **é™ä½ç‹€æ…‹è¤‡é›œåº¦**ï¼šå°‡ AgentState ç°¡åŒ–ç‚ºåƒ…åŒ…å«å¿…è¦æ¬„ä½
2. **ç²¾ç°¡åœ–çµæ§‹**ï¼šåƒ… 3 å€‹ç¯€é» (agent -> tools -> response_builder) ç›¸è¼ƒæ–¼åŸç‰ˆçš„ 7+ å€‹
3. **LLM è‡ªä¸»æ€§**ï¼šç§»é™¤è¤‡é›œè·¯ç”±é‚è¼¯ï¼Œè®“ LLM æ±ºå®šä½¿ç”¨å“ªäº›å·¥å…·
4. **ç§»é™¤ç›£ç£æ¶æ§‹**ï¼šç§»é™¤ supervisor_copywritingã€nlg_composeã€colloquialize ç¯€é»
5. **ç°¡åŒ–æ±ºç­–é‚è¼¯**ï¼šåŸºæ–¼å·¥å…·å¾ªç’°å’Œå·¥å…·å‘¼å«çš„åŸºæœ¬ should_continue()
6. **ç§»é™¤è¤‡é›œåŠŸèƒ½**ï¼šç„¡å°è©±æ­·å²ã€æœƒè©±ç®¡ç†æˆ–é€²éš NLG
7. **éŒ¯èª¤è™•ç†**ï¼šæœå‹™ä¸å¯ç”¨æ™‚çš„å„ªé›…é™ç´š
8. **ç›´æ¥å·¥å…·å­˜å–**ï¼šLLM å¯ç›´æ¥å‘¼å«ä»»ä½•å¯ç”¨å·¥å…·ï¼Œç„¡è·¯ç”±é™åˆ¶

æ¶æ§‹ï¼š
- å…¥å£ï¼šagent_nodeï¼ˆåˆå§‹åŒ–è¨Šæ¯ï¼Œä½¿ç”¨å·¥å…·å‘¼å« LLMï¼‰
- æ¢ä»¶ï¼šshould_continueï¼ˆæª¢æŸ¥å·¥å…·å‘¼å«å’Œå¾ªç’°é™åˆ¶ï¼‰
- å·¥å…·ï¼šToolNodeï¼ˆåŸ·è¡Œå·¥å…·å‘¼å«ï¼‰
- å‡ºå£ï¼šresponse_builderï¼ˆæ ¼å¼åŒ–æœ€çµ‚å›æ‡‰ï¼‰

æ­¤ç°¡åŒ–ç‰ˆæœ¬ä¿æŒæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒæ™‚æ›´å®¹æ˜“ç†è§£å’Œä¿®æ”¹ã€‚
LLM æ“æœ‰å®Œå…¨è‡ªä¸»æ¬Šï¼Œå¯æ ¹æ“šä½¿ç”¨è€…æŸ¥è©¢æ±ºå®šä½¿ç”¨å“ªäº›å·¥å…·ã€‚
"""
import logging
from typing import Dict, Any, List, Optional, Literal, TypedDict, Annotated
from datetime import datetime
import json
import os
import sys
from pathlib import Path

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain.callbacks.tracers import LangChainTracer
from langgraph.config import get_stream_writer

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥è·¯å¾‘
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
sys.path.append(project_root)

# å¸¶éŒ¯èª¤è™•ç†çš„æœå‹™å°å…¥
try:
    from app.settings import settings
    SETTINGS_AVAILABLE = True
    print(f"è¨­å®šå·²è¼‰å…¥ - OpenAI API Key: {'å·²è¨­å®š' if settings.openai_api_key else 'æœªè¨­å®š'}")
except ImportError as e:
    print(f"è­¦å‘Šï¼šç„¡æ³•è¼‰å…¥è¨­å®šï¼š{e}")
    SETTINGS_AVAILABLE = False

    # æ¨¡æ“¬è¨­å®š
    class MockSettings:
        openai_api_key = None
        azure_openai_api_key = None
        azure_openai_endpoint = None
        azure_openai_deployment = "gpt-4"
        azure_openai_api_version = "2023-12-01-preview"
        max_tool_loops = 3
        execute_tools = 1

    settings = MockSettings()

try:
    from app.services.fmp_client import fmp_client
    from app.services.line_client import line_client
    from app.services.file_ingest import file_ingest_service
    from app.services.rag import rag_service
    from app.services.report import report_service
    SERVICES_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Šï¼šéƒ¨åˆ†æœå‹™ä¸å¯ç”¨ï¼š{e}")
    SERVICES_AVAILABLE = False

    # ç‚ºç¼ºå¤±çš„æœå‹™å»ºç«‹æ¨¡æ“¬ç‰©ä»¶
    class MockService:
        async def get_quote(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def get_profile(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def get_news(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def get_macro_data(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def process_file(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def query_documents(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def answer_question(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def generate_report(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}
        async def fetch_messages(self, *args, **kwargs):
            return {"ok": False, "reason": "service_unavailable"}

    fmp_client = MockService()
    line_client = MockService()
    file_ingest_service = MockService()
    rag_service = MockService()
    report_service = MockService()

logger = logging.getLogger(__name__)


# ç°¡åŒ–ç‹€æ…‹å®šç¾©
class SimpleAgentState(TypedDict):
    """ç°¡åŒ–ä»£ç†ç‹€æ…‹ - åƒ…å°ˆæ³¨æ–¼å¿…è¦æ¬„ä½"""
    messages: Annotated[List[BaseMessage], add_messages]
    input_type: Literal["text", "file", "line", "rule"]
    query: Optional[str]
    file_info: Optional[Dict[str, Any]]
    line_info: Optional[Dict[str, Any]]
    rule_info: Optional[Dict[str, Any]]

    # å¿…è¦è™•ç†æ¬„ä½
    tool_results: Optional[List[Dict[str, Any]]]
    final_response: Optional[Dict[str, Any]]
    warnings: List[str]

    # å¾ªç’°æ§åˆ¶
    tool_loop_count: int


# å·¥å…·å®šç¾©ï¼ˆé‡ç”¨è‡ªåŸç‰ˆï¼‰
@tool
async def tool_fmp_quote(symbols: List[str]) -> Dict[str, Any]:
    """å–å¾—è‚¡ç¥¨å ±åƒ¹"""
    logger.info(f"å‘¼å« FMP å ±åƒ¹å·¥å…·ï¼š{symbols}")
    return await fmp_client.get_quote(symbols)


@tool
async def tool_fmp_profile(symbols: List[str]) -> Dict[str, Any]:
    """å–å¾—å…¬å¸è³‡æ–™"""
    logger.info(f"å‘¼å« FMP å…¬å¸è³‡æ–™å·¥å…·ï¼š{symbols}")
    return await fmp_client.get_profile(symbols)


@tool
async def tool_fmp_news(symbols: Optional[List[str]] = None,
                       query: Optional[str] = None,
                       limit: int = 10) -> Dict[str, Any]:
    """å–å¾—æ–°èè³‡æ–™"""
    logger.info(f"å‘¼å« FMP æ–°èå·¥å…·ï¼šsymbols={symbols}, query={query}")
    return await fmp_client.get_news(symbols=symbols, query=query, limit=limit)


@tool
async def tool_fmp_macro(indicator: str, country: str = "US") -> Dict[str, Any]:
    """å–å¾—ç¸½é«”ç¶“æ¿Ÿè³‡æ–™
    åƒæ•¸:
    - indicator: ç¸½ç¶“æŒ‡æ¨™åç¨±ï¼Œå¿…é ˆæ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š
      GDP, realGDP, nominalPotentialGDP, realGDPPerCapita, federalFunds, CPI,
      inflationRate, inflation, retailSales, consumerSentiment, durableGoods,
      unemploymentRate, totalNonfarmPayroll, initialClaims, industrialProductionTotalIndex,
      newPrivatelyOwnedHousingUnitsStartedTotalUnits, totalVehicleSales, retailMoneyFunds,
      smoothedUSRecessionProbabilities, 3MonthOr90DayRatesAndYieldsCertificatesOfDeposit,
      commercialBankInterestRateOnCreditCardPlansAllAccounts, 30YearFixedRateMortgageAverage,
      15YearFixedRateMortgageAverage
    - country: åœ‹å®¶ä»£ç¢¼ï¼Œé è¨­ç‚º "US"
    """

    logger.info(f"å‘¼å« FMP ç¸½ç¶“å·¥å…·ï¼š{indicator}, {country}")
    return await fmp_client.get_macro_data(indicator, country)


@tool
async def tool_file_load(file_path: str) -> Dict[str, Any]:
    """è¼‰å…¥æª”æ¡ˆå…§å®¹"""
    logger.info(f"å‘¼å«æª”æ¡ˆè¼‰å…¥å·¥å…·ï¼š{file_path}")
    return await file_ingest_service.process_file(file_path)


@tool
async def tool_rag_query(question: str, top_k: int = 5) -> Dict[str, Any]:
    """RAG æŸ¥è©¢"""
    logger.info(f"å‘¼å« RAG æŸ¥è©¢å·¥å…·ï¼š{question}")
    query_result = await rag_service.query_documents(question, top_k)
    if query_result["ok"] and query_result["data"]["relevant_chunks"]:
        return await rag_service.answer_question(question, query_result["data"]["relevant_chunks"])
    return query_result


@tool
async def tool_report_generate(template_id: str, context: Dict[str, Any],
                              output_formats: List[str] = None) -> Dict[str, Any]:
    """ç”¢ç”Ÿå ±å‘Š"""
    logger.info(f"å‘¼å«å ±å‘Šç”¢ç”Ÿå·¥å…·ï¼š{template_id}")
    output_formats = output_formats or ["markdown", "pdf"]
    return await report_service.generate_report(template_id, context, output_formats[0])


@tool
async def tool_line_fetch(user_id: Optional[str] = None,
                         chat_id: Optional[str] = None,
                         start_date: Optional[str] = None,
                         end_date: Optional[str] = None,
                         limit: int = 100) -> Dict[str, Any]:
    """æŠ“å– LINE è¨Šæ¯"""
    logger.info(f"å‘¼å« LINE æŠ“å–å·¥å…·ï¼šuser_id={user_id}, chat_id={chat_id}")
    return await line_client.fetch_messages(user_id, chat_id, start_date, end_date, limit)


# å·¥å…·æ¸…å–®
tools = [
    tool_fmp_quote,
    tool_fmp_profile,
    tool_fmp_news,
    tool_fmp_macro,
    tool_file_load,
    tool_rag_query,
    tool_report_generate,
    tool_line_fetch
]


class SimpleAgentGraph:
    """ç°¡åŒ–ä»£ç†åœ– - LLM é©…å‹•ï¼Œæœ€å°è·¯ç”±è¤‡é›œåº¦"""

    def __init__(self):
        self.llm = self._create_llm()
        self.graph = self._create_graph()
        self.tracer = LangChainTracer(project_name="agent") 

    def _create_llm(self) -> Optional[ChatOpenAI]:
        """å»ºç«‹ LLM å¯¦ä¾‹"""
        if settings.openai_api_key:
            return ChatOpenAI(
                model="gpt-4o-mini",
                api_key=settings.openai_api_key,
                temperature=0.3
            )
        elif settings.azure_openai_api_key and settings.azure_openai_endpoint:
            return ChatOpenAI(
                model=settings.azure_openai_deployment,
                api_key=settings.azure_openai_api_key,
                azure_endpoint=settings.azure_openai_endpoint,
                api_version=settings.azure_openai_api_version,
                temperature=0.3
            )
        else:
            logger.warning("æœªè¨­å®š OpenAI æˆ– Azure OpenAI API é‡‘é‘°")
            return None


    def _create_graph(self) -> StateGraph:
        """å»ºç«‹ç°¡åŒ– LangGraph"""
        workflow = StateGraph(SimpleAgentState)

        # åŠ å…¥ç¯€é» - åƒ…ç°¡åŒ–ç‚ºå¿…è¦ç¯€é»
        workflow.add_node("agent", self.agent_node)
        workflow.add_node("tools", ToolNode(tools))
        workflow.add_node("response_builder", self.response_builder)

        # è¨­å®šå…¥å£é»
        workflow.set_entry_point("agent")

        # åŠ å…¥æ¢ä»¶é‚Š - ç°¡åŒ–æ±ºç­–é‚è¼¯
        workflow.add_conditional_edges(
            "agent",
            self.should_continue,
            {
                "continue": "tools",
                "end": "response_builder"
            }
        )

        # å·¥å…·å›åˆ°ä»£ç†é€²è¡Œæ½›åœ¨çš„é¡å¤–è™•ç†
        workflow.add_edge("tools", "agent")

        # å›æ‡‰å»ºæ§‹å™¨çµæŸæµç¨‹
        workflow.add_edge("response_builder", END)

        return workflow.compile()

    def should_continue(self, state: SimpleAgentState) -> str:
        """ç°¡åŒ–æ±ºç­–é‚è¼¯ - è®“ LLM æ±ºå®šä½•æ™‚ä½¿ç”¨å·¥å…·"""
        # æª¢æŸ¥å·¥å…·åŸ·è¡Œé™åˆ¶
        tool_loop_count = state.get("tool_loop_count", 0)
        max_loops = getattr(settings, 'max_tool_loops', 3)

        if tool_loop_count >= max_loops:
            logger.info(f"é”åˆ°æœ€å¤§å·¥å…·å¾ªç’°æ¬¡æ•¸ï¼ˆ{max_loops}ï¼‰")
            return "end"

        # æª¢æŸ¥å·¥å…·æ˜¯å¦è¢«åœç”¨
        execute_tools = getattr(settings, 'execute_tools', 1)
        if isinstance(execute_tools, str):
            execute_tools = int(execute_tools)

        if execute_tools == 0:
            logger.info("å·¥å…·åŸ·è¡Œå·²åœç”¨")
            return "end"

        # æª¢æŸ¥æœ€å¾Œä¸€å‰‡è¨Šæ¯æ˜¯å¦æœ‰å·¥å…·å‘¼å«
        messages = state.get("messages", [])
        if messages:
            last_message = messages[-1]
            
            if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                return "continue"

        return "end"

    async def _handle_special_commands(self, state: SimpleAgentState) -> Optional[SimpleAgentState]:
        """è™•ç†ç‰¹æ®ŠæŒ‡ä»¤ (/report, /template)"""
        query = state.get("query", "").strip()

        if not query:
            return None

        # è™•ç† /report æŒ‡ä»¤
        if query.startswith("/report"):
            return await self._handle_report_command(state, query)

        # è™•ç† /template æŒ‡ä»¤
        elif query.startswith("/template"):
            return await self._handle_template_command(state, query)

        return None

    async def _handle_report_command(self, state: SimpleAgentState, query: str) -> SimpleAgentState:
        """è™•ç† /report æŒ‡ä»¤"""
        try:
            # æå–å ±å‘Šåƒæ•¸
            report_params = query[7:].strip()  # ç§»é™¤ "/report" å‰ç¶´

            if not report_params:
                error_msg = AIMessage(content="éŒ¯èª¤ï¼š/report æŒ‡ä»¤éœ€è¦åƒæ•¸ã€‚ä½¿ç”¨æ–¹å¼ï¼š/report AAPL NVDA è‚¡ç¥¨åˆ†æ")
                state["messages"].append(error_msg)
                return state

            # å°å…¥ä¸¦å¯¦ä¾‹åŒ– SimpleReportAgent
            from app.graphs.report_agent_simple import SimpleReportAgent
            report_agent = SimpleReportAgent()

            # æº–å‚™ ReportAgent è¼¸å…¥
            report_input = {
                "input_type": "text",
                "query": f"/report {report_params}",
                "session_id": state.get("session_id", "simple_agent"),
                "trace_id": f"simple_agent_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            }

            # åŸ·è¡Œå ±å‘Šç”Ÿæˆ
            logger.info(f"åŸ·è¡Œå ±å‘Šç”Ÿæˆï¼š{report_params}")
            report_result = await report_agent.run(report_input)

            # æ•´åˆçµæœåˆ°æ¨™æº–å›æ‡‰æ ¼å¼
            if report_result.get("ok", False):
                success_msg = AIMessage(content=report_result["response"])
                state["messages"].append(success_msg)

                # æ·»åŠ å ±å‘Šçµæœåˆ° tool_results
                if not state.get("tool_results"):
                    state["tool_results"] = []
                state["tool_results"].append({
                    "ok": True,
                    "type": "report_generation",
                    "output_files": report_result.get("output_files", []),
                    "session_id": report_result.get("session_id"),
                    "timestamp": report_result.get("timestamp")
                })
            else:
                error_msg = AIMessage(content=f"å ±å‘Šç”Ÿæˆå¤±æ•—ï¼š{report_result.get('response', 'æœªçŸ¥éŒ¯èª¤')}")
                state["messages"].append(error_msg)

                # æ·»åŠ éŒ¯èª¤åˆ° warnings
                if not state.get("warnings"):
                    state["warnings"] = []
                state["warnings"].append(f"å ±å‘Šç”Ÿæˆå¤±æ•—ï¼š{report_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

        except Exception as e:
            logger.error(f"è™•ç† /report æŒ‡ä»¤æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            error_msg = AIMessage(content=f"è™•ç†å ±å‘ŠæŒ‡ä»¤æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            state["messages"].append(error_msg)

            if not state.get("warnings"):
                state["warnings"] = []
            state["warnings"].append(f"å ±å‘ŠæŒ‡ä»¤è™•ç†éŒ¯èª¤ï¼š{str(e)}")

        return state

    async def _handle_template_command(self, state: SimpleAgentState, query: str) -> SimpleAgentState:
        """è™•ç† /template æŒ‡ä»¤"""
        try:
            # æå–æ¨¡æ¿ ID
            template_id = query[9:].strip()  # ç§»é™¤ "/template" å‰ç¶´

            if not template_id: 
                error_msg = AIMessage(content="éŒ¯èª¤ï¼š/template æŒ‡ä»¤éœ€è¦æ¨¡æ¿ IDã€‚ä½¿ç”¨æ–¹å¼ï¼š/template stock.j2")
                state["messages"].append(error_msg)
                return state

            # é©—è­‰æ¨¡æ¿æª”æ¡ˆ
            template_path = Path(project_root) / "templates" / "reports" / template_id

            # æª¢æŸ¥å‰¯æª”å
            if not (template_id.endswith('.j2') or template_id.endswith('.md')):
                error_msg = AIMessage(content=f"éŒ¯èª¤ï¼šæ¨¡æ¿æª”æ¡ˆå¿…é ˆæ˜¯ .j2 æˆ– .md æ ¼å¼ã€‚æ”¶åˆ°ï¼š{template_id}")
                state["messages"].append(error_msg)
                return state

            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            if not template_path.exists():
                available_templates = []
                templates_dir = Path(project_root) / "templates" / "reports"
                if templates_dir.exists():
                    available_templates = [f.name for f in templates_dir.glob("*.j2")] + [f.name for f in templates_dir.glob("*.md")]

                error_msg = AIMessage(content=f"éŒ¯èª¤ï¼šæ¨¡æ¿æª”æ¡ˆä¸å­˜åœ¨ï¼š{template_id}\nå¯ç”¨çš„æ¨¡æ¿ï¼š{', '.join(available_templates) if available_templates else 'ç„¡'}")
                state["messages"].append(error_msg)
                return state

            # æ›´æ–°å…¨åŸŸæ¨¡æ¿è¨­å®šï¼ˆé€šéç’°å¢ƒè®Šæ•¸ï¼‰
            os.environ["REPORT_TEMPLATE_ID"] = template_id

            # ç¢ºèªå›æ‡‰
            success_msg = AIMessage(content=f"âœ… æ¨¡æ¿å·²æˆåŠŸåˆ‡æ›ç‚ºï¼š{template_id}\næ¨¡æ¿è·¯å¾‘ï¼š{template_path}\næ­¤è¨­å®šå°‡ç”¨æ–¼å¾ŒçºŒçš„å ±å‘Šç”Ÿæˆã€‚")
            state["messages"].append(success_msg)

            logger.info(f"æ¨¡æ¿å·²åˆ‡æ›ç‚ºï¼š{template_id}")

        except Exception as e:
            logger.error(f"è™•ç† /template æŒ‡ä»¤æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            error_msg = AIMessage(content=f"è™•ç†æ¨¡æ¿æŒ‡ä»¤æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            state["messages"].append(error_msg)

            if not state.get("warnings"):
                state["warnings"] = []
            state["warnings"].append(f"æ¨¡æ¿æŒ‡ä»¤è™•ç†éŒ¯èª¤ï¼š{str(e)}")

        return state

    async def agent_node(self, state: SimpleAgentState) -> SimpleAgentState:
        """ç°¡åŒ–ä»£ç†ç¯€é» - è®“ LLM è™•ç†æ‰€æœ‰æ±ºç­–"""
        logger.info("åŸ·è¡Œç°¡åŒ–ä»£ç†ç¯€é»")
        messages = state.get("messages", [])
        writer = get_stream_writer()

        # æ ¹æ“šè¼¸å…¥é¡å‹åˆå§‹åŒ–è¨Šæ¯ï¼ˆå¦‚æœç‚ºç©ºï¼‰
        if not messages:
            messages = self._initialize_messages(state)
            state["messages"] = messages

        # å„ªå…ˆè™•ç†ç‰¹æ®ŠæŒ‡ä»¤ (/report, /template)
        special_result = await self._handle_special_commands(state)
        if special_result is not None:
            logger.info("ç‰¹æ®ŠæŒ‡ä»¤å·²è™•ç†ï¼Œè·³éä¸€èˆ¬ LLM è™•ç†æµç¨‹")
            return special_result

        # æª¢æŸ¥æ˜¯å¦å‰›å®Œæˆå·¥å…·åŸ·è¡Œ
        if self._has_tool_results(messages):
            current_count = state.get("tool_loop_count", 0)
            state["tool_loop_count"] = current_count + 1
            logger.info(f"å·¥å…·å¾ªç’°è¨ˆæ•¸ï¼š{state['tool_loop_count']}")

        # å¦‚æœ LLM ä¸å¯ç”¨ï¼Œæä¾›é™ç´šå›æ‡‰
        if not self.llm:
            logger.warning("LLM ä¸å¯ç”¨ï¼Œä½¿ç”¨é™ç´šå›æ‡‰")
            fallback_msg = AIMessage(content="LLM æœªè¨­å®šã€‚è«‹è¨­å®š OpenAI API é‡‘é‘°ã€‚")
            state["messages"].append(fallback_msg)
            return state

        # è®“ LLM æ±ºå®šä¸‹ä¸€æ­¥ - ç„¡è¤‡é›œè·¯ç”±
        try:
            llm_with_tools = self.llm.bind_tools(tools)
            # response = await llm_with_tools.ainvoke(messages)

            # ä½¿ç”¨ astream é€²è¡Œä¸²æµè™•ç†ï¼Œæ”¯æ´å·¥å…·å‘¼å«
            response_content = ""
            first = True
            gathered = None
            tool_calls_sent = False
            
            async for chunk in llm_with_tools.astream(messages):
                # ç´¯ç©è¨Šæ¯å¡Šä»¥è™•ç†å·¥å…·å‘¼å«
                if first:
                    gathered = chunk
                    first = False
                else:
                    gathered = gathered + chunk
                
                # ä¸²æµå…§å®¹åˆ° writer
                if hasattr(chunk, 'content') and chunk.content:
                    writer(chunk.content)
                    response_content += chunk.content
                
                # æª¢æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„å·¥å…·å‘¼å«ä¸”å°šæœªç™¼é€
                if (hasattr(gathered, 'tool_calls') and gathered.tool_calls and 
                    not tool_calls_sent and 
                    all(tc.get('args') and isinstance(tc.get('args'), dict) for tc in gathered.tool_calls)):
                    print("!tool_calls", gathered.tool_calls)

                #   writer(f"\n[å·¥å…·å‘¼å«] {len(gathered.tool_calls)} å€‹å·¥å…·")

                    # æ ¼å¼åŒ–å·¥å…·å‘¼å«è³‡è¨Š
                    tool_info = []
                    for tc in gathered.tool_calls:
                        tool_name = tc.get('name', 'unknown')
                        tool_args = tc.get('args', {})
                        tool_info.append(f"{tool_name}å·¥å…·è¢«å‘¼å«ï¼Œåƒæ•¸ç‚º{tool_args}\n\n")
                    
                    writer(f"\n[å·¥å…·å‘¼å«] {', '.join(tool_info)}")
                    tool_calls_sent = True
            
            # ä½¿ç”¨ç´¯ç©çš„è¨Šæ¯ä½œç‚ºå›æ‡‰ï¼ˆåŒ…å«å·¥å…·å‘¼å«ï¼‰
            response = gathered if gathered else AIMessage(content=response_content)
            
            state["messages"].append(response)
            tool_calls_count = len(getattr(response, 'tool_calls', []))
            logger.info(f"LLM å›æ‡‰å·²ç”¢ç”Ÿï¼ŒåŒ…å« {tool_calls_count} å€‹å·¥å…·å‘¼å«")
        except Exception as e:
            logger.error(f"LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
            error_msg = AIMessage(content=f"è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            state["messages"].append(error_msg)

        return state

    def _initialize_messages(self, state: SimpleAgentState) -> List[BaseMessage]:
        """æ ¹æ“šè¼¸å…¥é¡å‹å’Œç°¡åŒ–ç³»çµ±æç¤ºåˆå§‹åŒ–è¨Šæ¯"""
        input_type = state.get("input_type", "text")
        query = state.get("query", "")

        # é¼“å‹µ LLM è‡ªä¸»æ€§çš„ç°¡å–®ç³»çµ±æç¤º
        system_prompt = """ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„é‡‘èå’Œè³‡æ–™åˆ†æåŠ©ç†ã€‚ä½ å¯ä»¥ä½¿ç”¨å„ç¨®å·¥å…·ï¼š
            - è‚¡ç¥¨å ±åƒ¹å’Œé‡‘èè³‡æ–™ (tool_fmp_quote, tool_fmp_profile, tool_fmp_news, tool_fmp_macro)
            - æª”æ¡ˆè™•ç† (tool_file_load)
            - æ–‡ä»¶æœå°‹ (tool_rag_query)
            - å ±å‘Šç”¢ç”Ÿ (tool_report_generate)
            - LINE è¨Šæ¯æŠ“å– (tool_line_fetch)

            åˆ†æä½¿ç”¨è€…çš„è«‹æ±‚ä¸¦æ±ºå®šä½¿ç”¨å“ªäº›å·¥å…·ã€‚å¦‚æœ‰éœ€è¦ï¼Œä½ å¯ä»¥å‘¼å«å¤šå€‹å·¥å…·ã€‚åœ¨æ±ºç­–æ™‚è¦ä¿æŒè‡ªä¸»æ€§ï¼Œè‹¥æŸ¥è©¢ç¸½ç¶“ç›¸é—œå•é¡Œï¼Œè‡³å°‘éœ€èª¿ç”¨3å€‹ä¸åŒç¸½ç¶“æŒ‡æ¨™ã€‚

        """

        messages = [SystemMessage(content=system_prompt)]

        if input_type == "text":
            if query:
                messages.append(HumanMessage(content=query))
            else:
                messages.append(HumanMessage(content="ä»Šå¤©æˆ‘å¯ä»¥å¦‚ä½•å¹«åŠ©æ‚¨ï¼Ÿ"))

        elif input_type == "file":
            file_info = state.get("file_info", {})
            file_path = file_info.get("path", "")
            task = file_info.get("task", "qa")

            if task == "qa" and query:
                content = f"è«‹è™•ç†æª”æ¡ˆ '{file_path}' ä¸¦å›ç­”ï¼š{query}"
            else:
                content = f"è«‹è™•ç†æª”æ¡ˆ '{file_path}' åŸ·è¡Œä»»å‹™ï¼š{task}"
            messages.append(HumanMessage(content=content))

        elif input_type == "line":
            line_info = state.get("line_info", {})
            content = f"è«‹æŠ“å–ä¸¦åˆ†æ LINE è¨Šæ¯ï¼š{line_info}"
            messages.append(HumanMessage(content=content))

        elif input_type == "rule":
            rule_info = state.get("rule_info", {})
            content = f"è«‹åŸ·è¡Œè¦å‰‡æŸ¥è©¢ï¼š{rule_info}"
            messages.append(HumanMessage(content=content))

        return messages

    def _has_tool_results(self, messages: List[BaseMessage]) -> bool:
        """æª¢æŸ¥è¨Šæ¯æ˜¯å¦åŒ…å«æœ€è¿‘çš„å·¥å…·åŸ·è¡Œçµæœ"""
        if len(messages) < 2:
            return False

        # å°‹æ‰¾æ¨¡å¼ï¼šå¸¶æœ‰ tool_calls çš„ AIMessage å¾Œè·Ÿ ToolMessage(s)
        for i in range(len(messages) - 1):
            msg = messages[i]
            if (hasattr(msg, "tool_calls") and msg.tool_calls and
                i + 1 < len(messages) and
                isinstance(messages[i + 1], ToolMessage)):
                return True
        return False

    async def response_builder(self, state: SimpleAgentState) -> SimpleAgentState:
        """å¾å°è©±å’Œå·¥å…·çµæœå»ºæ§‹æœ€çµ‚å›æ‡‰"""
        logger.info("å»ºæ§‹æœ€çµ‚å›æ‡‰")
        writer = get_stream_writer()
        messages = state.get("messages", [])
        tool_results = []
        warnings = state.get("warnings", [])

        # å¾è¨Šæ¯ä¸­æå–å·¥å…·çµæœ
        for msg in messages:
            if isinstance(msg, ToolMessage):
                try:
                    if isinstance(msg.content, str):
                        content = json.loads(msg.content)
                    else:
                        content = msg.content
                    tool_results.append(content)
                except (json.JSONDecodeError, TypeError):
                    tool_results.append({"raw_content": str(msg.content)})

        # å–å¾—æœ€çµ‚ AI å›æ‡‰
        final_ai_response = ""
        for msg in reversed(messages):
            if isinstance(msg, AIMessage) and not msg.tool_calls:
                final_ai_response = msg.content
                # writer(msg.content)
                break

        # å¦‚æœæ²’æœ‰æœ€çµ‚å›æ‡‰ï¼Œä½¿ç”¨ LLM ç”¢ç”Ÿè‡ªç„¶èªè¨€å›æ‡‰
        # if not final_ai_response and tool_results:
        #     print("generate natural response")
        #     final_ai_response = await self._generate_natural_response(state, tool_results)
        # elif not final_ai_response:
        #     final_ai_response = "æˆ‘æº–å‚™å¥½å¹«åŠ©æ‚¨ï¼è«‹å‘Šè¨´æˆ‘æ‚¨éœ€è¦ä»€éº¼ã€‚"

        # å»ºæ§‹æœ€çµ‚å›æ‡‰
        state["final_response"] = {
            "ok": True,
            "response": final_ai_response,
            "input_type": state["input_type"],
            "tool_results": tool_results,
            "warnings": warnings,
            "tool_loop_count": state.get("tool_loop_count", 0),
            "timestamp": datetime.now().isoformat(),
            "simplified": True  # æ¨™è¨˜ç‚ºç°¡åŒ–ç‰ˆæœ¬
        }

        state["tool_results"] = tool_results
        return state

    # async def _generate_natural_response(self, state: SimpleAgentState, tool_results: List[Dict[str, Any]]) -> str:
    #     """ä½¿ç”¨ LLM ç”¢ç”Ÿè‡ªç„¶èªè¨€å›æ‡‰"""
    #     try:
    #         # å¦‚æœ LLM ä¸å¯ç”¨ï¼Œä½¿ç”¨é™ç´šå›æ‡‰
    #         writer = get_stream_writer()
    #         if not self.llm:
    #             successful_tools = sum(1 for tr in tool_results if isinstance(tr, dict) and tr.get("ok", False))
    #             return f"åŸ·è¡Œäº† {len(tool_results)} å€‹å·¥å…·ï¼ˆ{successful_tools} å€‹æˆåŠŸï¼‰ã€‚è«‹æŸ¥çœ‹ tool_results å–å¾—è©³ç´°è³‡è¨Šã€‚"

    #         # æº–å‚™å·¥å…·çµæœæ‘˜è¦
    #         tool_summary = self._prepare_tool_summary(tool_results)

    #         # å–å¾—åŸå§‹æŸ¥è©¢
    #         original_query = state.get("query", "")

    #         # å»ºç«‹æç¤ºä¾†ç”¢ç”Ÿè‡ªç„¶å›æ‡‰
    #         response_prompt = f"""åŸºæ–¼ä»¥ä¸‹å·¥å…·åŸ·è¡Œçµæœï¼Œè«‹ç‚ºä½¿ç”¨è€…æä¾›ä¸€å€‹æ¸…æ™°ã€æœ‰ç”¨çš„å›æ‡‰ã€‚

    #         åŸå§‹æŸ¥è©¢ï¼š{original_query}

    #         å·¥å…·åŸ·è¡Œçµæœï¼š
    #         {tool_summary}

    #         è«‹æä¾›ä¸€å€‹è‡ªç„¶ã€ç›´æ¥çš„å›æ‡‰ï¼Œå°ˆæ³¨æ–¼å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚ä¸è¦æåŠæŠ€è¡“ç´°ç¯€å¦‚ã€Œå·¥å…·åŸ·è¡Œã€æˆ–ã€ŒAPI å‘¼å«ã€ã€‚
    #         å¦‚æœæœ‰å…·é«”çš„è³‡æ–™ï¼ˆå¦‚è‚¡åƒ¹ã€æ–°èç­‰ï¼‰ï¼Œè«‹ç›´æ¥å‘ˆç¾çµ¦ä½¿ç”¨è€…ã€‚
    #         å¦‚æœå·¥å…·åŸ·è¡Œå¤±æ•—ï¼Œè«‹ç¦®è²Œåœ°èªªæ˜ç„¡æ³•å–å¾—è³‡è¨Šçš„åŸå› ã€‚"""

    #         # ä½¿ç”¨ LLM ç”¢ç”Ÿå›æ‡‰
    #         # response = self.llm.invoke([HumanMessage(content=response_prompt)])
    #         print(response_prompt)
            
    #         # ä½¿ç”¨ astream é€²è¡Œä¸²æµè™•ç†
    #         response_content = ""
    #         async for chunk in self.llm.astream([HumanMessage(content=response_prompt)]):
    #             print(chunk)
    #             if hasattr(chunk, 'content') and chunk.content:
    #                 writer(chunk.content)
    #                 response_content += chunk.content
            
    #         return response_content.strip()

    #     except Exception as e:
    #         logger.error(f"ç”¢ç”Ÿè‡ªç„¶èªè¨€å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    #         # é™ç´šåˆ°æŠ€è¡“æ€§æ‘˜è¦
    #         successful_tools = sum(1 for tr in tool_results if isinstance(tr, dict) and tr.get("ok", False))
    #         return f"åŸ·è¡Œäº† {len(tool_results)} å€‹å·¥å…·ï¼ˆ{successful_tools} å€‹æˆåŠŸï¼‰ã€‚è«‹æŸ¥çœ‹ tool_results å–å¾—è©³ç´°è³‡è¨Šã€‚"

    # def _prepare_tool_summary(self, tool_results: List[Dict[str, Any]]) -> str:
    #     """æº–å‚™å·¥å…·çµæœæ‘˜è¦ä¾› LLM ä½¿ç”¨"""
    #     if not tool_results:
    #         return "æ²’æœ‰å·¥å…·åŸ·è¡Œçµæœã€‚"

    #     summary_parts = []
    #     for i, result in enumerate(tool_results, 1):
    #         if isinstance(result, dict):
    #             if result.get("ok", False):
    #                 # æˆåŠŸçš„å·¥å…·çµæœ
    #                 data = result.get("data", {})
    #                 source = result.get("source", "æœªçŸ¥ä¾†æº")

    #                 if isinstance(data, list) and data:
    #                     # è™•ç†åˆ—è¡¨è³‡æ–™ï¼ˆå¦‚è‚¡ç¥¨å ±åƒ¹ï¼‰
    #                     summary_parts.append(f"å·¥å…· {i} ({source}) æˆåŠŸï¼š{json.dumps(data, ensure_ascii=False, indent=2)}")
    #                 elif isinstance(data, dict):
    #                     # è™•ç†å­—å…¸è³‡æ–™
    #                     summary_parts.append(f"å·¥å…· {i} ({source}) æˆåŠŸï¼š{json.dumps(data, ensure_ascii=False, indent=2)}")
    #                 else:
    #                     # è™•ç†å…¶ä»–è³‡æ–™é¡å‹
    #                     summary_parts.append(f"å·¥å…· {i} ({source}) æˆåŠŸï¼š{str(data)}")
    #             else:
    #                 # å¤±æ•—çš„å·¥å…·çµæœ
    #                 reason = result.get("reason", "æœªçŸ¥éŒ¯èª¤")
    #                 error = result.get("error", "")
    #                 summary_parts.append(f"å·¥å…· {i} å¤±æ•—ï¼š{reason} {error}".strip())
    #         else:
    #             # è™•ç†éå­—å…¸çµæœ
    #             summary_parts.append(f"å·¥å…· {i} çµæœï¼š{str(result)}")

    #     return "\n".join(summary_parts)

    async def run(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œç°¡åŒ–ä»£ç†"""
        try:
            # æº–å‚™åˆå§‹ç‹€æ…‹
            initial_state = SimpleAgentState(
                messages=[],
                input_type=input_data["input_type"],
                query=input_data.get("query"),
                file_info=input_data.get("file"),
                line_info=input_data.get("line"),
                rule_info=input_data.get("rule"),
                tool_results=None,
                final_response=None,
                warnings=[],
                tool_loop_count=0
            )

            # åŸ·è¡Œåœ–
            config = {"recursion_limit": 10, "callbacks": [self.tracer]}
            # result = await self.graph.ainvoke(initial_state, config=config)
            async for result in self.graph.astream(initial_state,subgraphs=False,stream_mode=["custom"],config=config):
                yield result

            # return result["final_response"]

        except Exception as e:
            logger.error(f"ç°¡åŒ–ä»£ç†åŸ·è¡Œå¤±æ•—ï¼š{str(e)}", exc_info=True)
            yield "error"
            # return {
            #     "ok": False,
            #     "error": str(e),
            #     "input_type": input_data.get("input_type", "unknown"),
            #     "timestamp": datetime.now().isoformat(),
            #     "simplified": True
            # }

    async def run_sync(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œç°¡åŒ–ä»£ç†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨æ–¼æ¸¬è©¦ï¼‰"""
        try:
            # æº–å‚™åˆå§‹ç‹€æ…‹
            initial_state = SimpleAgentState(
                messages=[],
                input_type=input_data["input_type"],
                query=input_data.get("query"),
                file_info=input_data.get("file"),
                line_info=input_data.get("line"),
                rule_info=input_data.get("rule"),
                tool_results=None,
                final_response=None,
                warnings=[],
                tool_loop_count=0
            )

            # åŸ·è¡Œåœ–
            config = {"recursion_limit": 10, "callbacks": [self.tracer]}
            result = await self.graph.ainvoke(initial_state, config=config)

            return result["final_response"]

        except Exception as e:
            logger.error(f"ç°¡åŒ–ä»£ç†åŸ·è¡Œå¤±æ•—ï¼š{str(e)}", exc_info=True)
            return {
                "ok": False,
                "error": str(e),
                "input_type": input_data.get("input_type", "unknown"),
                "timestamp": datetime.now().isoformat(),
                "simplified": True
            }


# å»ºç«‹å…¨åŸŸå¯¦ä¾‹
# simple_agent_graph = SimpleAgentGraph()


def build_simple_graph():
    """èˆ‡ç¾æœ‰ç¨‹å¼ç¢¼ç›¸å®¹çš„å»ºæ§‹å‡½æ•¸"""
    return simple_agent_graph.graph


# ç¾æœ‰ä»‹é¢çš„ç›¸å®¹æ€§å‡½æ•¸
def build_graph():
    """å›å‚³ç°¡åŒ–åœ–çš„ç›¸å®¹æ€§å‡½æ•¸"""
    return build_simple_graph()


if __name__ == "__main__":
    """ç›´æ¥åŸ·è¡Œæ”¯æ´"""
    import asyncio
    async def test_simple_agent():

        simple_agent_graph = SimpleAgentGraph()

        """ä½¿ç”¨ç¯„ä¾‹æŸ¥è©¢æ¸¬è©¦ç°¡åŒ–ä»£ç†"""
        try:
            input = {
                "input_type": "text",
                "query": "æœ€è¿‘AAPLæœ‰ä»€éº¼æ–°èï¼Ÿ"
                # "query": "ä½ å¥½"
                # "query": "/report AAPL"
            }
            async for result in simple_agent_graph.run(input):
                print(result[1],flush=True,end="")
            # print(f"âœ… æˆåŠŸï¼š{result.get('response', 'ç„¡å›æ‡‰')}")
            # if result.get('tool_results'):
                # print(f"ğŸ”§ åŸ·è¡Œçš„å·¥å…·ï¼š{len(result['tool_results'])}")
        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼š{str(e)}")

        print("-" * 30)
        print("\nğŸ‰ æ¸¬è©¦å®Œæˆï¼")

    asyncio.run(test_simple_agent())